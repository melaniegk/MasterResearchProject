{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1102854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "\n",
    "# Load your pre-split testing dataset into a pandas DataFrame\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_test = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_test.csv')\n",
    "\n",
    "df_train = df_train.drop('pt', axis=1)\n",
    "df_test = df_test.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_test = df_test.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_test = df_test['Neuropsychiatric symptoms-new']\n",
    "\n",
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5f2cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)',\n",
       "       'shallowSleepTime (hours)', 'Oxygen level (SpO2)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06719a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfccc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    424\n",
      "1    424\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=33)\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled.to_csv('oversampled_normalized_ML_COVID_train_byparticipants.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58870eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Model Accuracy (Test Set)</th>\n",
       "      <th>Cross-Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.859232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.853131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.861273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.861192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models  Model Accuracy (Test Set)  Cross-Validation Accuracy\n",
       "3    Voting Classifier                      0.872                   0.859232\n",
       "2  Logistic Regression                      0.856                   0.853131\n",
       "0        Random Forest                      0.848                   0.861273\n",
       "1              XGBoost                      0.848                   0.861192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=30)\n",
    "xgb_classifier = XGBClassifier(random_state=30)\n",
    "logreg_classifier = LogisticRegression(random_state=30)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest',\n",
    "         'XGBoost',\n",
    "         'Logistic Regression',\n",
    "         'Voting Classifier']\n",
    "\n",
    "model_accuracy_test_set = []\n",
    "model_accuracy_cross_val = []\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    # Evaluate on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test_set = model.predict(X_test)\n",
    "    test_set_score = accuracy_score(y_test, y_pred_test_set)\n",
    "    model_accuracy_test_set.append(test_set_score)\n",
    "\n",
    "    # Evaluate using cross-validation on the training set\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=32)\n",
    "    cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cross_val_score_mean = cross_val_results.mean()\n",
    "    model_accuracy_cross_val.append(cross_val_score_mean)\n",
    "\n",
    "evaluation = pd.DataFrame({\n",
    "    'Models': names,\n",
    "    'Model Accuracy (Test Set)': model_accuracy_test_set,\n",
    "    'Cross-Validation Accuracy': model_accuracy_cross_val\n",
    "})\n",
    "\n",
    "evaluation.sort_values(by=\"Model Accuracy (Test Set)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efe7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8720\n"
     ]
    }
   ],
   "source": [
    "firstmodel = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')\n",
    "firstmodel.fit(X_train,y_train)\n",
    "pred = firstmodel.predict(X_test)\n",
    "print('Accuracy on test set: {:.4f}'.format(firstmodel.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72971d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       107\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.86       125\n",
      "   macro avg       0.43      0.50      0.46       125\n",
      "weighted avg       0.73      0.86      0.79       125\n",
      "\n",
      "[[107   0]\n",
      " [ 18   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "LRmodel = LogisticRegression(random_state=30)\n",
    "LRmodel.fit(X_train,y_train)\n",
    "LRpred = LRmodel.predict(X_test)\n",
    "print('Accuracy on test set: {:.4f}'.format(LRmodel.score(X_test, y_test)))\n",
    "print(classification_report(y_test, LRpred))\n",
    "print(confusion_matrix(y_test, LRpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334aa3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "#dataframe\n",
    "CM_LR = confusion_matrix(y_test,LRpred)\n",
    "df_LR = pd.DataFrame(CM_LR)\n",
    "df_LR\n",
    "ax = sn.heatmap(df_LR,annot=True,fmt='.20g',cmap='Blues')\n",
    "ax.set_title('Confusion Matrix - Logistic Regression')\n",
    "ax.set_xlabel('Predict') #x\n",
    "ax.set_ylabel('True') #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e6c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'logistic_regression__C': 0.1, 'random_forest__n_estimators': 100, 'xgboost__n_estimators': 100}\n",
      "Test Set Accuracy: 0.856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have X_train, y_train, X_test, y_test from your dataset\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=67)\n",
    "xgb_classifier = XGBClassifier(random_state=67)\n",
    "logreg_classifier = LogisticRegression(random_state=67)\n",
    "\n",
    "# List of models\n",
    "models = [\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "]\n",
    "\n",
    "# Create a VotingClassifier\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='hard')\n",
    "\n",
    "# Define hyperparameters for each individual classifier\n",
    "param_grid = {\n",
    "    'random_forest__n_estimators': [50, 100, 200],\n",
    "    'xgboost__n_estimators': [50, 100, 200, 300],\n",
    "    'logistic_regression__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "# Perform Grid Search on the hyperparameters\n",
    "grid_search = GridSearchCV(estimator=voting_classifier, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the tuned model on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b90b6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best VotingClassifier on the test set: 0.856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters for each classifier\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "params_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Initialize the classifiers with default parameters\n",
    "rf_classifier = RandomForestClassifier(random_state=66)\n",
    "xgb_classifier = XGBClassifier(random_state=66)\n",
    "logreg_classifier = LogisticRegression(random_state=66)\n",
    "\n",
    "# GridSearchCV for each classifier\n",
    "grid_rf = GridSearchCV(rf_classifier, params_rf, cv=5, scoring='accuracy')\n",
    "grid_xgb = GridSearchCV(xgb_classifier, params_xgb, cv=5, scoring='accuracy')\n",
    "grid_logreg = GridSearchCV(logreg_classifier, params_logreg, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search models\n",
    "grid_rf.fit(X_train, y_train)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters for each classifier\n",
    "best_params_rf = grid_rf.best_params_\n",
    "best_params_xgb = grid_xgb.best_params_\n",
    "best_params_logreg = grid_logreg.best_params_\n",
    "\n",
    "# Use the best estimators obtained from grid search\n",
    "best_rf = grid_rf.best_estimator_\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "best_logreg = grid_logreg.best_estimator_\n",
    "\n",
    "# Create a VotingClassifier with the best estimators\n",
    "best_voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', best_rf),\n",
    "    ('xgboost', best_xgb),\n",
    "    ('logistic_regression', best_logreg)\n",
    "], voting='hard')\n",
    "\n",
    "# Fit the VotingClassifier with the best estimators\n",
    "best_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best VotingClassifier\n",
    "best_voting_accuracy = best_voting_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy of the best VotingClassifier on the test set:\", best_voting_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f283b557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 100}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ecd8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617cda04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61bf8fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5, random_state=66)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, random_state=66)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=5, random_state=66)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2f1e6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=66, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=66, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=66, ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4c33c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, random_state=66, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, random_state=66, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, random_state=66, solver='liblinear')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c478517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       107\n",
      "           1       0.67      0.22      0.33        18\n",
      "\n",
      "    accuracy                           0.87       125\n",
      "   macro avg       0.77      0.60      0.63       125\n",
      "weighted avg       0.85      0.87      0.84       125\n",
      "\n",
      "[[105   2]\n",
      " [ 14   4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Score evaluation of tuned GBC final model\n",
    "finalVC = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')\n",
    "\n",
    "finalVC.fit(X_train,y_train)\n",
    "VCpred = finalVC.predict(X_test)\n",
    "\n",
    "print('Accuracy on test set: {:.4f}'.format(finalVC.score(X_test, y_test)))\n",
    "print(classification_report(y_test, VCpred))\n",
    "print(confusion_matrix(y_test, VCpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6589c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "#dataframe\n",
    "CM_VC = confusion_matrix(y_test,VCpred)\n",
    "df_VC = pd.DataFrame(CM_VC)\n",
    "df_VC\n",
    "ax = sn.heatmap(df_VC,annot=True,fmt='.20g',cmap='Blues')\n",
    "ax.set_title('Confusion Matrix - Voting Classifier')\n",
    "ax.set_xlabel('Predict') #x\n",
    "ax.set_ylabel('True') #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ee41077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Analysis of actual values vs predicted values based on tuned VC model\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, VCpred)\n",
    "cm = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20476cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # Your data and plotting code\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Create the bar plot\n",
    "# sns.barplot(x=evaluation[\"Models\"], y=evaluation[\"Model Accuracy (Test Set)\"])\n",
    "\n",
    "# # Adding values on top of each bar\n",
    "# for index, value in enumerate(evaluation[\"Model Accuracy (Test Set)\"]):\n",
    "#     plt.text(index, value + 0.01, f'{value:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# # Adding lines to the y-axis\n",
    "# for index, value in enumerate(evaluation[\"Model Accuracy (Test Set)\"]):\n",
    "#     plt.plot([index, index], [0, value], color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# plt.title(\"Models Accuracy Score Comparison\", size=15)\n",
    "# plt.xticks(rotation=0, size=12)\n",
    "# plt.show()\n",
    "\n",
    "# #visualization of model scores\n",
    "# plt.figure(figsize = (10, 6))\n",
    "# sns.barplot(x = evaluation[\"Models\"], y = evaluation[\"Model Accuracy (Test Set)\"])\n",
    "# plt.title(\"Models Accuracy Score Comparison\", size = 15)\n",
    "# plt.xticks(rotation = 0, size = 12)\n",
    "# plt.show()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x=evaluation[\"Models\"], y=evaluation[\"Model Accuracy (Test Set)\"])\n",
    "\n",
    "# Add lines connecting each bar to the y-axis and display values on top as percentages\n",
    "for index, value in enumerate(evaluation[\"Model Accuracy (Test Set)\"]):\n",
    "    plt.text(index, value + 0.01, f'{value:.2%}', ha='center')  # Display value on top of each bar in percentage\n",
    "    plt.axhline(y=value, xmax=index, color='blue', linestyle='--', linewidth=0.5)  # Vertical dashed line to y-axis\n",
    "#     plt.hlines(y=value, xmin=-0.4, xmax=index, colors='blue', linestyle='--', linewidth=0.5)  # Line connecting bar to y-axis\n",
    "\n",
    "plt.title(\"Models Accuracy Score Comparison\", size=15)\n",
    "plt.xticks(rotation=0, size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cce555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "\n",
    "# 'pt' is a column that identifies participants\n",
    "participants = df['pt'].unique()\n",
    "\n",
    "# Number of participants\n",
    "# num_participants = len(data)\n",
    "\n",
    "# Number of folds\n",
    "num_folds = 6\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=30)\n",
    "xgb_classifier = XGBClassifier(random_state=30)\n",
    "logreg_classifier = LogisticRegression(random_state=30)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest',\n",
    "         'XGBoost',\n",
    "         'Logistic Regression',\n",
    "         'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_indices, val_indices in skf.split(data):\n",
    "    # Split data into training and validation sets\n",
    "    train_data = [data[i] for i in train_indices]\n",
    "    val_data = data[val_indices[0]]  # Take the first participant as the validation set\n",
    "\n",
    "    # Split data into features and labels (replace with your actual data preprocessing)\n",
    "    X_train, y_train = preprocess_data(train_data)\n",
    "    X_val, y_val = preprocess_data(val_data)\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        # Train your model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training and validation sets\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        # Store accuracies\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Training Accuracies: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies: {val_accuracies[name]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "\n",
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=40)\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61324f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 'pt' is a column that identifies participants, 'target' is the target variable\n",
    "participants = df['pt'].unique()\n",
    "targets = df.groupby('pt')['target'].max()  # Assuming 'target' is the column containing class labels\n",
    "\n",
    "# Number of folds\n",
    "num_folds = 6\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=34)\n",
    "xgb_classifier = XGBClassifier(random_state=34)\n",
    "logreg_classifier = LogisticRegression(random_state=34)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest',\n",
    "         'XGBoost',\n",
    "         'Logistic Regression',\n",
    "         'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Cross-validation loop with Stratified K-Fold\n",
    "for train_indices, val_indices in skf.split(participants, targets):\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_participants = participants[train_indices]\n",
    "    val_participants = participants[val_indices]\n",
    "\n",
    "    train_data = df[df['pt'].isin(train_participants)]\n",
    "    val_data = df[df['pt'].isin(val_participants)]\n",
    "\n",
    "    # Split data into features and labels (replace with your actual data preprocessing)\n",
    "    X_train, y_train = preprocess_data(train_data)\n",
    "    X_val, y_val = preprocess_data(val_data)\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        # Train your model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training and validation sets\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        # Store accuracies\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Training Accuracies: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa4af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    424\n",
      "1    424\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n",
      "Random Forest - Training Accuracies: [0.9971671388101983, 0.9971671388101983, 0.9985855728429985, 0.9971711456859972, 1.0, 0.9971711456859972]\n",
      "Random Forest - Validation Accuracies: [0.9577464788732394, 0.9577464788732394, 0.9858156028368794, 0.9787234042553191, 0.900709219858156, 0.9645390070921985]\n",
      "\n",
      "XGBoost - Training Accuracies: [0.9971671388101983, 0.9971671388101983, 0.9985855728429985, 0.9971711456859972, 1.0, 0.9971711456859972]\n",
      "XGBoost - Validation Accuracies: [0.971830985915493, 0.9577464788732394, 0.9787234042553191, 0.9645390070921985, 0.950354609929078, 0.9574468085106383]\n",
      "\n",
      "Logistic Regression - Training Accuracies: [0.5509915014164306, 0.5424929178470255, 0.5148514851485149, 0.5332390381895332, 0.5332390381895332, 0.5685997171145686]\n",
      "Logistic Regression - Validation Accuracies: [0.528169014084507, 0.5704225352112676, 0.5460992907801419, 0.4397163120567376, 0.5177304964539007, 0.48936170212765956]\n",
      "\n",
      "Voting Classifier - Training Accuracies: [0.9971671388101983, 0.9971671388101983, 0.9985855728429985, 0.9971711456859972, 1.0, 0.9971711456859972]\n",
      "Voting Classifier - Validation Accuracies: [0.9577464788732394, 0.9647887323943662, 0.9858156028368794, 0.9716312056737588, 0.950354609929078, 0.950354609929078]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "\n",
    "column_names = ['pt', 'deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)', 'shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Separate features (X) and target variable (y) for the training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=40)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Number of folds\n",
    "num_folds = 6\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=34)\n",
    "xgb_classifier = XGBClassifier(random_state=34)\n",
    "logreg_classifier = LogisticRegression(random_state=34)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Cross-validation loop with Stratified K-Fold\n",
    "for train_indices, val_indices in skf.split(df_resampled.drop('Neuropsychiatric symptoms-new', axis=1), df_resampled['Neuropsychiatric symptoms-new']):\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train = df_resampled.iloc[train_indices].drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_train = df_resampled.iloc[train_indices]['Neuropsychiatric symptoms-new']\n",
    "    \n",
    "    X_val = df_resampled.iloc[val_indices].drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_val = df_resampled.iloc[val_indices]['Neuropsychiatric symptoms-new']\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        # Train your model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training and validation sets\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        # Store accuracies\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Training Accuracies: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228ca3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    424\n",
      "1    424\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "\n",
    "df_train = df_train.drop('pt', axis=1)\n",
    "\n",
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)', 'shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Separate features (X) and target variable (y) for the training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=40)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled.to_csv('oversampled_normalized_ML_COVID_train_byparticipants_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8f4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Participants for Training: [11]\n",
      "Fold 1 - Participants for Validation: [15]\n",
      "\n",
      "Fold 2 - Participants for Training: [16]\n",
      "Fold 2 - Participants for Validation: [18]\n",
      "\n",
      "Fold 3 - Participants for Training: [19]\n",
      "Fold 3 - Participants for Validation: [1]\n",
      "\n",
      "Fold 4 - Participants for Training: [2]\n",
      "Fold 4 - Participants for Validation: [5]\n",
      "\n",
      "Fold 5 - Participants for Training: [8]\n",
      "Fold 5 - Participants for Validation: [9]\n",
      "\n",
      "Fold 6 - Participants for Training: [3]\n",
      "Fold 6 - Participants for Validation: [17]\n",
      "\n",
      "Random Forest - Training Accuracies: [1.0, 1.0, 1.0, 1.0, 1.0, 0.9782608695652174]\n",
      "Random Forest - Validation Accuracies: [0.45588235294117646, 0.4222222222222222, 0.6585365853658537, 0.5205479452054794, 0.37362637362637363, 0.48314606741573035]\n",
      "\n",
      "XGBoost - Training Accuracies: [1.0, 0.9726027397260274, 0.9880952380952381, 1.0, 1.0, 0.9565217391304348]\n",
      "XGBoost - Validation Accuracies: [0.4411764705882353, 0.3333333333333333, 0.5853658536585366, 0.5205479452054794, 0.3626373626373626, 0.5280898876404494]\n",
      "\n",
      "Logistic Regression - Training Accuracies: [0.7804878048780488, 0.726027397260274, 0.6309523809523809, 0.7288135593220338, 0.5913978494623656, 0.6739130434782609]\n",
      "Logistic Regression - Validation Accuracies: [0.4411764705882353, 0.5555555555555556, 0.7073170731707317, 0.4520547945205479, 0.34065934065934067, 0.5842696629213483]\n",
      "\n",
      "Voting Classifier - Training Accuracies: [1.0, 1.0, 0.9880952380952381, 1.0, 1.0, 0.9565217391304348]\n",
      "Voting Classifier - Validation Accuracies: [0.4411764705882353, 0.4, 0.6341463414634146, 0.5205479452054794, 0.3626373626373626, 0.550561797752809]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "# Define participant groups for each fold\n",
    "participant_groups = [\n",
    "    [11, 15],  # Fold 1\n",
    "    [16, 18],  # Fold 2\n",
    "    [19, 1],   # Fold 3\n",
    "    [2, 5],    # Fold 4\n",
    "    [8, 9],    # Fold 5\n",
    "    [3, 17]    # Fold 6 (Validation)\n",
    "]\n",
    "\n",
    "# Number of folds\n",
    "num_folds = len(participant_groups)\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=34)\n",
    "xgb_classifier = XGBClassifier(random_state=34)\n",
    "logreg_classifier = LogisticRegression(random_state=34)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Cross-validation loop with custom participant groups\n",
    "for fold, (train_participants, val_participants) in enumerate(participant_groups):\n",
    "\n",
    "    # Convert single integer values to lists\n",
    "    train_participants = [train_participants] if isinstance(train_participants, int) else train_participants\n",
    "    val_participants = [val_participants] if isinstance(val_participants, int) else val_participants\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data = df_resampled[df_resampled['pt'].isin(train_participants)]\n",
    "    val_data = df_resampled[df_resampled['pt'].isin(val_participants)]\n",
    "\n",
    "    # Split data into features and labels for training set\n",
    "    X_train = train_data.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_train = train_data['Neuropsychiatric symptoms-new']\n",
    "\n",
    "    # Split data into features and labels for validation set\n",
    "    X_val = val_data.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_val = val_data['Neuropsychiatric symptoms-new']\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        model.fit(X_train, y_train)\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Participants for Training: {train_participants}\")\n",
    "    print(f\"Fold {fold + 1} - Participants for Validation: {val_participants}\")\n",
    "    print()\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Training Accuracies: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae4fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Average Training Accuracy: 0.996376811594203\n",
      "Random Forest - Average Validation Accuracy: 0.4856602577961393\n",
      "Random Forest - Training Accuracies for each fold: [1.0, 1.0, 1.0, 1.0, 1.0, 0.9782608695652174]\n",
      "Random Forest - Validation Accuracies for each fold: [0.45588235294117646, 0.4222222222222222, 0.6585365853658537, 0.5205479452054794, 0.37362637362637363, 0.48314606741573035]\n",
      "\n",
      "XGBoost - Average Training Accuracy: 0.9862032861586166\n",
      "XGBoost - Average Validation Accuracy: 0.4618584755105661\n",
      "XGBoost - Training Accuracies for each fold: [1.0, 0.9726027397260274, 0.9880952380952381, 1.0, 1.0, 0.9565217391304348]\n",
      "XGBoost - Validation Accuracies for each fold: [0.4411764705882353, 0.3333333333333333, 0.5853658536585366, 0.5205479452054794, 0.3626373626373626, 0.5280898876404494]\n",
      "\n",
      "Logistic Regression - Average Training Accuracy: 0.6885986725588941\n",
      "Logistic Regression - Average Validation Accuracy: 0.5135054829026265\n",
      "Logistic Regression - Training Accuracies for each fold: [0.7804878048780488, 0.726027397260274, 0.6309523809523809, 0.7288135593220338, 0.5913978494623656, 0.6739130434782609]\n",
      "Logistic Regression - Validation Accuracies for each fold: [0.4411764705882353, 0.5555555555555556, 0.7073170731707317, 0.4520547945205479, 0.34065934065934067, 0.5842696629213483]\n",
      "\n",
      "Voting Classifier - Average Training Accuracy: 0.9907694962042788\n",
      "Voting Classifier - Average Validation Accuracy: 0.48484498627455014\n",
      "Voting Classifier - Training Accuracies for each fold: [1.0, 1.0, 0.9880952380952381, 1.0, 1.0, 0.9565217391304348]\n",
      "Voting Classifier - Validation Accuracies for each fold: [0.4411764705882353, 0.4, 0.6341463414634146, 0.5205479452054794, 0.3626373626373626, 0.550561797752809]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "# Define participant groups for each fold\n",
    "participant_groups = [\n",
    "    [11, 15],  # Fold 1\n",
    "    [16, 18],  # Fold 2\n",
    "    [19, 1],   # Fold 3\n",
    "    [2, 5],    # Fold 4\n",
    "    [8, 9],    # Fold 5\n",
    "    [3, 17]    # Fold 6 (Validation)\n",
    "]\n",
    "\n",
    "# Number of folds\n",
    "num_folds = len(participant_groups)\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=34)\n",
    "xgb_classifier = XGBClassifier(random_state=34)\n",
    "logreg_classifier = LogisticRegression(random_state=34)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Cross-validation loop with custom participant groups\n",
    "for fold, (train_participants, val_participants) in enumerate(participant_groups):\n",
    "    \n",
    "    # Convert single integer values to lists\n",
    "    train_participants = [train_participants] if isinstance(train_participants, int) else train_participants\n",
    "    val_participants = [val_participants] if isinstance(val_participants, int) else val_participants\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data = df_resampled[df_resampled['pt'].isin(train_participants)]\n",
    "    val_data = df_resampled[df_resampled['pt'].isin(val_participants)]\n",
    "\n",
    "    # Split data into features and labels for training set\n",
    "    X_train = train_data.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_train = train_data['Neuropsychiatric symptoms-new']\n",
    "\n",
    "    # Split data into features and labels for validation set\n",
    "    X_val = val_data.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "    y_val = val_data['Neuropsychiatric symptoms-new']\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        model.fit(X_train, y_train)\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Average Training Accuracy: {sum(train_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Average Validation Accuracy: {sum(val_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Training Accuracies for each fold: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies for each fold: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e817d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Participants for Training: [  0   1   3   4   5   6   7   8   9  10  12  14  15  16  17  18  20  21\n",
      "  22  23  24  25  26  27  29  31  32  33  34  35  36  37  38  39  40  42\n",
      "  43  44  45  46  48  49  50  51  52  53  55  56  57  58  59  61  62  64\n",
      "  65  66  67  68  69  70  71  72  74  75  76  77  79  80  81  82  83  84\n",
      "  87  88  89  90  92  96  98  99 100 101 102 103 104 105 108 109 110 111\n",
      " 113 114 115 118 120 121 123 124 125 126 127 128 129 132 133 134 135 136\n",
      " 137 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 173 174\n",
      " 175 176 177 179 180 181 183 184 185 186 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199 200 201 202 203 205 207 208 209 210 211 212 213 214 215\n",
      " 216 220 221 222 223 224 225 226 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 255 257\n",
      " 258 259 260 261 262 263 264 265 266 267 269 270 271 272 273 274 275 276\n",
      " 277 278 279 280 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
      " 296 297 298 299 300 301 303 305 306 308 309 311 314 315 316 317 318 319\n",
      " 320 321 322 323 325 326 327 328 329 330 331 334 336 337 338 339 340 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 357 359 360 361 362 364 365\n",
      " 366 367 368 369 370 372 373 374 376 378 380 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 403 404 405 406 407\n",
      " 410 412 413 415 416 417 419 420 421 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 436 438 439 440 441 442 445 446 447 448 450 452 453 454 455\n",
      " 456 457 458 460 461 462 463 464 465 466 467 468 469 471 472 473 474 475\n",
      " 476 477 478 479 480 481 482 483 484 485 486 487 489 490 491 492 493 494\n",
      " 495 496 497 498 499 500 501 502 505 506 508 509 510 511 512 513 514 515\n",
      " 516 517 518 520 521 522 523 524 525 526 527 530 531 532 533 534 535 537\n",
      " 538 539 540 541 543 544 545 546 547 548 549 550 551 552 553 556 557 558\n",
      " 559 562 565 567 568 569 570 571 572 574 575 576 577 578 579 580 582 583\n",
      " 584 585 586 588 591 592 593 594 595 597 598 599 600 601 602 603 604 606\n",
      " 608 609 610 611 614 615 616 618 619 620 621 622 623 624 625 626 627 628\n",
      " 629 630 631 632 633 635 636 637 638 639 640 641 642 643 644 645 647 648\n",
      " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
      " 667 669 670 672 674 675 676 677 678 679 680 681 684 685 686 687 688 689\n",
      " 691 692 693 695 696 697 698 699 700 701 702 703 704 705 707 708 709 711\n",
      " 712 715 716 717 719 720 721 722 723 724 725 726 728 729 730 732 733 734\n",
      " 735 737 738 740 741 742 743 744 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 767 768 769 770 772 775 776 778\n",
      " 779 780 781 783 784 785 786 787 788 789 790 791 794 795 796 797 798 799\n",
      " 800 801 802 803 804 806 808 809 810 811 812 814 815 816 817 818 820 821\n",
      " 822 823 824 825 826 827 828 829 830 832 833 834 835 836 837 838 840 841\n",
      " 842 843 844 846]\n",
      "Fold 1 - Participants for Validation: [  2  11  13  19  28  30  41  47  54  60  63  73  78  85  86  91  93  94\n",
      "  95  97 106 107 112 116 117 119 122 130 131 138 172 178 182 187 204 206\n",
      " 217 218 219 227 254 256 268 281 302 304 307 310 312 313 324 332 333 335\n",
      " 341 354 355 356 358 363 371 375 377 379 381 402 408 409 411 414 418 422\n",
      " 435 437 443 444 449 451 459 470 488 503 504 507 519 528 529 536 542 554\n",
      " 555 560 561 563 564 566 573 581 587 589 590 596 605 607 612 613 617 634\n",
      " 646 668 671 673 682 683 690 694 706 710 713 714 718 727 731 736 739 745\n",
      " 766 771 773 774 777 782 792 793 805 807 813 819 831 839 845 847]\n",
      "\n",
      "Fold 2 - Participants for Training: [  0   1   2   3   4   5   6  11  13  15  17  18  19  20  21  22  23  24\n",
      "  25  26  27  28  29  30  31  32  34  36  37  38  39  40  41  42  43  46\n",
      "  47  49  50  51  52  53  54  55  56  58  59  60  63  64  65  66  67  71\n",
      "  73  74  75  76  77  78  79  80  82  83  84  85  86  87  89  90  91  92\n",
      "  93  94  95  97  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 124 125 127 128 129 130 131 132\n",
      " 133 135 137 138 140 143 144 146 147 148 149 150 151 153 154 155 156 158\n",
      " 159 161 163 164 165 166 167 168 169 170 171 172 174 175 176 177 178 180\n",
      " 181 182 183 184 185 186 187 188 191 192 193 194 195 196 197 198 200 201\n",
      " 202 203 204 206 207 208 210 212 213 214 216 217 218 219 220 222 223 225\n",
      " 226 227 228 229 230 231 232 233 236 237 238 240 241 242 243 244 245 246\n",
      " 247 248 249 250 251 252 254 255 256 257 258 259 260 262 263 264 265 266\n",
      " 267 268 270 271 272 273 274 275 276 277 278 280 281 282 284 285 286 289\n",
      " 290 291 293 295 296 297 299 300 301 302 303 304 306 307 308 309 310 312\n",
      " 313 314 315 316 317 320 321 324 325 326 327 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 346 347 348 349 350 352 353 354 355\n",
      " 356 357 358 359 360 361 363 364 365 366 367 368 369 371 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 408 409 410 411 413 414 415\n",
      " 417 418 419 420 421 422 423 424 426 427 428 429 431 432 433 434 435 436\n",
      " 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 472 473 474\n",
      " 475 477 478 480 481 482 483 486 487 488 489 493 494 497 499 500 501 503\n",
      " 504 505 506 507 508 509 511 512 513 514 515 516 517 518 519 521 523 524\n",
      " 527 528 529 531 532 533 535 536 537 542 544 545 546 547 548 549 550 551\n",
      " 552 553 554 555 557 558 560 561 562 563 564 566 567 568 569 571 572 573\n",
      " 574 575 576 577 579 580 581 582 583 584 585 586 587 588 589 590 591 592\n",
      " 593 594 595 596 597 598 599 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 622 623 624 625 626 627 628 629 632\n",
      " 633 634 635 637 638 640 642 643 644 646 647 648 649 650 651 652 653 654\n",
      " 655 656 657 658 659 660 662 663 665 666 667 668 669 671 672 673 674 675\n",
      " 676 677 678 679 681 682 683 684 685 686 687 689 690 691 692 694 695 696\n",
      " 697 698 699 700 701 703 704 705 706 707 709 710 711 712 713 714 715 716\n",
      " 717 718 719 720 721 722 723 724 725 726 727 728 729 731 732 733 734 735\n",
      " 736 737 738 739 740 741 742 743 744 745 747 748 749 750 751 752 753 756\n",
      " 757 760 762 764 765 766 767 768 769 771 772 773 774 775 776 777 778 779\n",
      " 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798\n",
      " 799 800 801 802 803 804 805 806 807 808 809 810 811 813 814 815 816 817\n",
      " 819 820 821 822 823 825 826 828 829 830 831 832 833 834 835 836 837 839\n",
      " 843 844 845 847]\n",
      "Fold 2 - Participants for Validation: [  7   8   9  10  12  14  16  33  35  44  45  48  57  61  62  68  69  70\n",
      "  72  81  88  96  98 123 126 134 136 139 141 142 145 152 157 160 162 173\n",
      " 179 189 190 199 205 209 211 215 221 224 234 235 239 253 261 269 279 283\n",
      " 287 288 292 294 298 305 311 318 319 322 323 328 345 351 362 370 372 373\n",
      " 407 412 416 425 430 452 471 476 479 484 485 490 491 492 495 496 498 502\n",
      " 510 520 522 525 526 530 534 538 539 540 541 543 556 559 565 570 578 600\n",
      " 621 630 631 636 639 641 645 661 664 670 680 688 693 702 708 730 746 754\n",
      " 755 758 759 761 763 770 780 812 818 824 827 838 840 841 842 846]\n",
      "\n",
      "Fold 3 - Participants for Training: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  22  25  26  27  28  29  30  31  32  33  34  35  38  40  41  43  44\n",
      "  45  46  47  48  49  51  52  53  54  55  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  71  72  73  74  76  77  78  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 103 106 107 108 109 110 111 112 113 114 115 116 117 118 119 122 123 124\n",
      " 125 126 127 128 129 130 131 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 147 148 151 152 157 160 162 163 164 165 166 167 168 169 170 171\n",
      " 172 173 174 176 177 178 179 180 182 183 184 187 188 189 190 191 192 194\n",
      " 195 196 197 198 199 200 204 205 206 207 209 210 211 213 214 215 216 217\n",
      " 218 219 221 222 224 227 228 229 230 231 232 233 234 235 236 238 239 240\n",
      " 241 242 243 244 245 246 248 249 250 251 252 253 254 255 256 257 258 260\n",
      " 261 264 265 266 267 268 269 270 272 273 274 275 276 277 278 279 281 282\n",
      " 283 285 286 287 288 289 290 291 292 293 294 295 297 298 299 300 301 302\n",
      " 303 304 305 306 307 310 311 312 313 314 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 331 332 333 334 335 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 351 352 353 354 355 356 357 358 359 361 362 363 364\n",
      " 365 366 367 369 370 371 372 373 374 375 376 377 378 379 381 382 383 385\n",
      " 386 387 388 389 390 393 394 395 396 397 398 399 402 403 405 406 407 408\n",
      " 409 410 411 412 414 415 416 417 418 419 420 421 422 423 424 425 426 428\n",
      " 429 430 431 432 433 434 435 436 437 440 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 457 458 459 461 462 463 464 465 466 467 468 470 471 474\n",
      " 475 476 477 479 480 481 482 483 484 485 486 487 488 490 491 492 493 494\n",
      " 495 496 497 498 499 501 502 503 504 505 506 507 508 510 511 513 514 515\n",
      " 516 517 518 519 520 522 523 524 525 526 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 545 546 547 548 549 550 551 552 553 554\n",
      " 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572\n",
      " 573 575 576 577 578 580 581 582 585 587 588 589 590 591 592 593 594 595\n",
      " 596 599 600 601 602 603 604 605 607 609 610 612 613 614 615 616 617 618\n",
      " 619 620 621 623 624 625 626 627 628 629 630 631 634 636 638 639 641 642\n",
      " 643 644 645 646 648 650 651 652 653 656 657 658 659 661 664 665 666 668\n",
      " 669 670 671 672 673 675 676 677 678 679 680 682 683 685 687 688 690 691\n",
      " 692 693 694 695 696 698 699 700 701 702 703 704 706 708 709 710 711 712\n",
      " 713 714 715 716 717 718 719 721 722 723 724 725 726 727 729 730 731 732\n",
      " 733 734 735 736 738 739 740 743 744 745 746 748 749 751 752 753 754 755\n",
      " 756 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
      " 775 777 778 779 780 782 786 788 789 790 791 792 793 794 795 796 798 799\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 817 818 819\n",
      " 820 821 822 824 827 828 829 830 831 834 835 836 837 838 839 840 841 842\n",
      " 843 844 845 846 847]\n",
      "Fold 3 - Participants for Validation: [  0   3  21  23  24  36  37  39  42  50  75  79 102 104 105 120 121 132\n",
      " 146 149 150 153 154 155 156 158 159 161 175 181 185 186 193 201 202 203\n",
      " 208 212 220 223 225 226 237 247 259 262 263 271 280 284 296 308 309 315\n",
      " 316 330 336 350 360 368 380 384 391 392 400 401 404 413 427 438 439 441\n",
      " 442 455 456 460 469 472 473 478 489 500 509 512 521 527 544 574 579 583\n",
      " 584 586 597 598 606 608 611 622 632 633 635 637 640 647 649 654 655 660\n",
      " 662 663 667 674 681 684 686 689 697 705 707 720 728 737 741 742 747 750\n",
      " 757 776 781 783 784 785 787 797 800 816 823 825 826 832 833]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Participants for Training: [  0   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  21  23  24  26  28  29  30  31  33  34  35  36  37  38  39  41  42  43\n",
      "  44  45  46  47  48  49  50  52  53  54  55  56  57  60  61  62  63  65\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 104 105\n",
      " 106 107 108 109 110 112 113 115 116 117 118 119 120 121 122 123 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 166 167 168 169 170 171 172 173 174 175 177 178 179 180 181 182 183 184\n",
      " 185 186 187 189 190 191 193 194 197 198 199 201 202 203 204 205 206 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 223 224 225 226 227\n",
      " 228 229 232 234 235 237 238 239 240 243 244 245 246 247 249 250 251 252\n",
      " 253 254 255 256 257 258 259 261 262 263 264 265 266 268 269 270 271 272\n",
      " 273 274 275 277 279 280 281 282 283 284 287 288 290 291 292 293 294 295\n",
      " 296 298 299 300 301 302 303 304 305 307 308 309 310 311 312 313 314 315\n",
      " 316 317 318 319 320 322 323 324 326 327 328 329 330 331 332 333 334 335\n",
      " 336 339 340 341 343 344 345 347 350 351 352 354 355 356 358 359 360 361\n",
      " 362 363 364 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383 384 386 388 389 391 392 393 394 396 397 398 400 401 402 404 405\n",
      " 407 408 409 410 411 412 413 414 416 417 418 419 422 423 424 425 427 428\n",
      " 429 430 431 432 433 435 436 437 438 439 440 441 442 443 444 445 447 449\n",
      " 450 451 452 453 454 455 456 459 460 462 464 465 466 468 469 470 471 472\n",
      " 473 474 475 476 477 478 479 480 481 484 485 486 487 488 489 490 491 492\n",
      " 494 495 496 497 498 499 500 501 502 503 504 506 507 508 509 510 512 514\n",
      " 516 518 519 520 521 522 523 525 526 527 528 529 530 531 532 533 534 536\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 552 553 554 555 556\n",
      " 557 559 560 561 563 564 565 566 567 568 569 570 573 574 575 577 578 579\n",
      " 580 581 582 583 584 586 587 588 589 590 591 593 596 597 598 599 600 601\n",
      " 603 604 605 606 607 608 611 612 613 614 616 617 618 619 620 621 622 623\n",
      " 625 626 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 649 650 651 652 653 654 655 656 657 659 660 661 662 663\n",
      " 664 665 666 667 668 669 670 671 672 673 674 676 677 678 680 681 682 683\n",
      " 684 685 686 687 688 689 690 692 693 694 695 696 697 700 701 702 703 704\n",
      " 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 723\n",
      " 725 726 727 728 729 730 731 733 734 736 737 738 739 741 742 743 744 745\n",
      " 746 747 749 750 754 755 756 757 758 759 760 761 763 764 766 769 770 771\n",
      " 772 773 774 775 776 777 778 780 781 782 783 784 785 786 787 789 790 792\n",
      " 793 794 796 797 798 800 801 802 803 805 807 808 811 812 813 814 816 818\n",
      " 819 820 822 823 824 825 826 827 829 831 832 833 835 836 838 839 840 841\n",
      " 842 843 845 846 847]\n",
      "Fold 4 - Participants for Validation: [  1   4  20  22  25  27  32  40  51  58  59  64  66  67  80 103 111 114\n",
      " 124 163 164 165 176 188 192 195 196 200 207 222 230 231 233 236 241 242\n",
      " 248 260 267 276 278 285 286 289 297 306 321 325 337 338 342 346 348 349\n",
      " 353 357 365 366 385 387 390 395 399 403 406 415 420 421 426 434 446 448\n",
      " 457 458 461 463 467 482 483 493 505 511 513 515 517 524 535 537 551 558\n",
      " 562 571 572 576 585 592 594 595 602 609 610 615 624 627 648 658 675 679\n",
      " 691 698 699 722 724 732 735 740 748 751 752 753 762 765 767 768 779 788\n",
      " 791 795 799 804 806 809 810 815 817 821 828 830 834 837 844]\n",
      "\n",
      "Fold 5 - Participants for Training: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
      "  19  20  21  22  23  24  25  27  28  29  30  32  33  35  36  37  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59\n",
      "  60  61  62  63  64  66  67  68  69  70  71  72  73  74  75  76  78  79\n",
      "  80  81  84  85  86  88  89  90  91  92  93  94  95  96  97  98 100 102\n",
      " 103 104 105 106 107 109 110 111 112 113 114 116 117 119 120 121 122 123\n",
      " 124 125 126 129 130 131 132 133 134 136 137 138 139 140 141 142 144 145\n",
      " 146 147 149 150 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 169 172 173 175 176 178 179 181 182 183 185 186 187 188 189 190 191 192\n",
      " 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210\n",
      " 211 212 215 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 233 234 235 236 237 239 240 241 242 243 244 245 247 248 251 253 254 256\n",
      " 258 259 260 261 262 263 264 265 267 268 269 270 271 274 276 277 278 279\n",
      " 280 281 283 284 285 286 287 288 289 290 292 294 295 296 297 298 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 315 316 317 318 319 321 322\n",
      " 323 324 325 328 330 332 333 334 335 336 337 338 339 341 342 344 345 346\n",
      " 347 348 349 350 351 353 354 355 356 357 358 359 360 361 362 363 365 366\n",
      " 368 369 370 371 372 373 374 375 377 379 380 381 382 383 384 385 386 387\n",
      " 388 390 391 392 394 395 396 397 399 400 401 402 403 404 406 407 408 409\n",
      " 410 411 412 413 414 415 416 418 420 421 422 425 426 427 429 430 432 434\n",
      " 435 436 437 438 439 440 441 442 443 444 446 448 449 451 452 454 455 456\n",
      " 457 458 459 460 461 462 463 465 467 469 470 471 472 473 474 475 476 477\n",
      " 478 479 480 482 483 484 485 486 487 488 489 490 491 492 493 495 496 498\n",
      " 499 500 502 503 504 505 507 509 510 511 512 513 515 517 518 519 520 521\n",
      " 522 524 525 526 527 528 529 530 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 547 548 549 550 551 554 555 556 558 559 560 561 562 563 564\n",
      " 565 566 567 569 570 571 572 573 574 575 576 577 578 579 581 583 584 585\n",
      " 586 587 588 589 590 591 592 594 595 596 597 598 600 601 602 605 606 607\n",
      " 608 609 610 611 612 613 615 616 617 618 620 621 622 623 624 625 626 627\n",
      " 629 630 631 632 633 634 635 636 637 639 640 641 642 645 646 647 648 649\n",
      " 650 652 654 655 656 658 659 660 661 662 663 664 665 667 668 670 671 673\n",
      " 674 675 676 677 678 679 680 681 682 683 684 685 686 688 689 690 691 693\n",
      " 694 696 697 698 699 701 702 703 705 706 707 708 709 710 713 714 715 716\n",
      " 717 718 719 720 722 723 724 725 727 728 730 731 732 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 757 758\n",
      " 759 760 761 762 763 765 766 767 768 770 771 772 773 774 776 777 779 780\n",
      " 781 782 783 784 785 787 788 789 791 792 793 794 795 796 797 798 799 800\n",
      " 801 802 804 805 806 807 808 809 810 812 813 815 816 817 818 819 820 821\n",
      " 823 824 825 826 827 828 830 831 832 833 834 835 836 837 838 839 840 841\n",
      " 842 844 845 846 847]\n",
      "Fold 5 - Participants for Validation: [ 15  26  31  34  38  56  65  77  82  83  87  99 101 108 115 118 127 128\n",
      " 135 143 148 151 166 167 168 170 171 174 177 180 184 213 214 216 232 238\n",
      " 246 249 250 252 255 257 266 272 273 275 282 291 293 299 300 314 320 326\n",
      " 327 329 331 340 343 352 364 367 376 378 389 393 398 405 417 419 423 424\n",
      " 428 431 433 445 447 450 453 464 466 468 481 494 497 501 506 508 514 516\n",
      " 523 531 532 546 552 553 557 568 580 582 593 599 603 604 614 619 628 638\n",
      " 643 644 651 653 657 666 669 672 687 692 695 700 704 711 712 721 726 729\n",
      " 733 734 756 764 769 775 778 786 790 803 811 814 822 829 843]\n",
      "\n",
      "Fold 6 - Participants for Training: [  0   1   2   3   4   7   8   9  10  11  12  13  14  15  16  19  20  21\n",
      "  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  44  45  47  48  50  51  54  56  57  58  59  60  61  62  63  64\n",
      "  65  66  67  68  69  70  72  73  75  77  78  79  80  81  82  83  85  86\n",
      "  87  88  91  93  94  95  96  97  98  99 101 102 103 104 105 106 107 108\n",
      " 111 112 114 115 116 117 118 119 120 121 122 123 124 126 127 128 130 131\n",
      " 132 134 135 136 138 139 141 142 143 145 146 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 184 185 186 187 188 189 190 192 193\n",
      " 195 196 199 200 201 202 203 204 205 206 207 208 209 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 230 231 232 233 234 235\n",
      " 236 237 238 239 241 242 246 247 248 249 250 252 253 254 255 256 257 259\n",
      " 260 261 262 263 266 267 268 269 271 272 273 275 276 278 279 280 281 282\n",
      " 283 284 285 286 287 288 289 291 292 293 294 296 297 298 299 300 302 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 335 336 337 338 340 341 342 343\n",
      " 345 346 348 349 350 351 352 353 354 355 356 357 358 360 362 363 364 365\n",
      " 366 367 368 370 371 372 373 375 376 377 378 379 380 381 384 385 387 389\n",
      " 390 391 392 393 395 398 399 400 401 402 403 404 405 406 407 408 409 411\n",
      " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 430\n",
      " 431 433 434 435 437 438 439 441 442 443 444 445 446 447 448 449 450 451\n",
      " 452 453 455 456 457 458 459 460 461 463 464 466 467 468 469 470 471 472\n",
      " 473 476 478 479 481 482 483 484 485 488 489 490 491 492 493 494 495 496\n",
      " 497 498 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515\n",
      " 516 517 519 520 521 522 523 524 525 526 527 528 529 530 531 532 534 535\n",
      " 536 537 538 539 540 541 542 543 544 546 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 568 570 571 572 573 574 576 578 579 580\n",
      " 581 582 583 584 585 586 587 589 590 592 593 594 595 596 597 598 599 600\n",
      " 602 603 604 605 606 607 608 609 610 611 612 613 614 615 617 619 621 622\n",
      " 624 627 628 630 631 632 633 634 635 636 637 638 639 640 641 643 644 645\n",
      " 646 647 648 649 651 653 654 655 657 658 660 661 662 663 664 666 667 668\n",
      " 669 670 671 672 673 674 675 679 680 681 682 683 684 686 687 688 689 690\n",
      " 691 692 693 694 695 697 698 699 700 702 704 705 706 707 708 710 711 712\n",
      " 713 714 718 720 721 722 724 726 727 728 729 730 731 732 733 734 735 736\n",
      " 737 739 740 741 742 745 746 747 748 750 751 752 753 754 755 756 757 758\n",
      " 759 761 762 763 764 765 766 767 768 769 770 771 773 774 775 776 777 778\n",
      " 779 780 781 782 783 784 785 786 787 788 790 791 792 793 795 797 799 800\n",
      " 803 804 805 806 807 809 810 811 812 813 814 815 816 817 818 819 821 822\n",
      " 823 824 825 826 827 828 829 830 831 832 833 834 837 838 839 840 841 842\n",
      " 843 844 845 846 847]\n",
      "Fold 6 - Participants for Validation: [  5   6  17  18  29  43  46  49  52  53  55  71  74  76  84  89  90  92\n",
      " 100 109 110 113 125 129 133 137 140 144 147 169 183 191 194 197 198 210\n",
      " 228 229 240 243 244 245 251 258 264 265 270 274 277 290 295 301 303 317\n",
      " 334 339 344 347 359 361 369 374 382 383 386 388 394 396 397 410 429 432\n",
      " 436 440 454 462 465 474 475 477 480 486 487 499 518 533 545 547 548 549\n",
      " 550 567 569 575 577 588 591 601 616 618 620 623 625 626 629 642 650 652\n",
      " 656 659 665 676 677 678 685 696 701 703 709 715 716 717 719 723 725 738\n",
      " 743 744 749 760 772 789 794 796 798 801 802 808 820 835 836]\n",
      "\n",
      "Random Forest - Average Training Accuracy: 0.9945750240746453\n",
      "Random Forest - Average Validation Accuracy: 0.898611527319948\n",
      "Random Forest - Training Accuracies for each fold: [0.9943342776203966, 0.9929178470254958, 0.9957567185289957, 0.9929278642149929, 0.9957567185289957, 0.9957567185289957]\n",
      "Random Forest - Validation Accuracies for each fold: [0.8732394366197183, 0.9014084507042254, 0.900709219858156, 0.950354609929078, 0.8652482269503546, 0.900709219858156]\n",
      "\n",
      "XGBoost - Average Training Accuracy: 0.9919812398075097\n",
      "XGBoost - Average Validation Accuracy: 0.9210035627476443\n",
      "XGBoost - Training Accuracies for each fold: [0.9929178470254958, 0.9915014164305949, 0.9929278642149929, 0.9900990099009901, 0.9915134370579916, 0.9929278642149929]\n",
      "XGBoost - Validation Accuracies for each fold: [0.9154929577464789, 0.9154929577464789, 0.900709219858156, 0.9432624113475178, 0.9290780141843972, 0.9219858156028369]\n",
      "\n",
      "Logistic Regression - Average Training Accuracy: 0.5735809315451982\n",
      "Logistic Regression - Average Validation Accuracy: 0.5707638264575633\n",
      "Logistic Regression - Training Accuracies for each fold: [0.5651558073654391, 0.5651558073654391, 0.5601131541725601, 0.5770862800565771, 0.5657708628005658, 0.6082036775106082]\n",
      "Logistic Regression - Validation Accuracies for each fold: [0.5633802816901409, 0.5704225352112676, 0.6028368794326241, 0.5035460992907801, 0.6028368794326241, 0.5815602836879432]\n",
      "\n",
      "Voting Classifier - Average Training Accuracy: 0.9929245251518273\n",
      "Voting Classifier - Average Validation Accuracy: 0.9103985615822593\n",
      "Voting Classifier - Training Accuracies for each fold: [0.9929178470254958, 0.9929178470254958, 0.9943422913719944, 0.9915134370579916, 0.9929278642149929, 0.9929278642149929]\n",
      "Voting Classifier - Validation Accuracies for each fold: [0.8943661971830986, 0.9084507042253521, 0.900709219858156, 0.9432624113475178, 0.9078014184397163, 0.9078014184397163]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "# Define the target variable\n",
    "target_variable = 'Neuropsychiatric symptoms-new'\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=60)\n",
    "xgb_classifier = XGBClassifier(random_state=60)\n",
    "logreg_classifier = LogisticRegression(random_state=60)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Stratified K-Fold cross-validation loop\n",
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(df_resampled.drop(target_variable, axis=1), df_resampled[target_variable])):\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data = df_resampled.iloc[train_indices]\n",
    "    val_data = df_resampled.iloc[val_indices]\n",
    "\n",
    "    # Split data into features and labels for training set\n",
    "    X_train = train_data.drop(target_variable, axis=1)\n",
    "    y_train = train_data[target_variable]\n",
    "\n",
    "    # Split data into features and labels for validation set\n",
    "    X_val = val_data.drop(target_variable, axis=1)\n",
    "    y_val = val_data[target_variable]\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        model.fit(X_train, y_train)\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "        \n",
    "    print(f\"Fold {fold + 1} - Participants for Training: {train_indices}\")\n",
    "    print(f\"Fold {fold + 1} - Participants for Validation: {val_indices}\")\n",
    "    print()\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Average Training Accuracy: {sum(train_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Average Validation Accuracy: {sum(val_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Training Accuracies for each fold: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies for each fold: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters for each classifier\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.3, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "params_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2', 'none'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize the classifiers with default parameters\n",
    "rf_classifier = RandomForestClassifier(random_state=61)\n",
    "xgb_classifier = XGBClassifier(random_state=61)\n",
    "logreg_classifier = LogisticRegression(random_state=61)\n",
    "\n",
    "# GridSearchCV for each classifier\n",
    "grid_rf = GridSearchCV(rf_classifier, params_rf, cv=5, scoring='accuracy')\n",
    "grid_xgb = GridSearchCV(xgb_classifier, params_xgb, cv=5, scoring='accuracy')\n",
    "grid_logreg = GridSearchCV(logreg_classifier, params_logreg, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search models\n",
    "grid_rf.fit(X_train, y_train)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters for each classifier\n",
    "best_params_rf = grid_rf.best_params_\n",
    "best_params_xgb = grid_xgb.best_params_\n",
    "best_params_logreg = grid_logreg.best_params_\n",
    "\n",
    "# Use the best estimators obtained from grid search\n",
    "best_rf = grid_rf.best_estimator_\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "best_logreg = grid_logreg.best_estimator_\n",
    "\n",
    "# Create a VotingClassifier with the best estimators\n",
    "best_voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', best_rf),\n",
    "    ('xgboost', best_xgb),\n",
    "    ('logistic_regression', best_logreg)\n",
    "], voting='hard')\n",
    "\n",
    "# Fit the VotingClassifier with the best estimators\n",
    "best_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best VotingClassifier\n",
    "best_voting_accuracy = best_voting_classifier.score(X_val, y_val)\n",
    "print(\"Accuracy of the best VotingClassifier on the test set:\", best_voting_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bc550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.53946659        nan 0.52804915 0.53087604 0.53087604 0.5294576\n",
      "        nan 0.50144841 0.49714314 0.49433623        nan 0.49714314\n",
      " 0.4900909  0.48725402 0.49571471        nan 0.50144841 0.49714314\n",
      " 0.50428529        nan 0.49573469 0.49863151 0.49578464 0.49714314\n",
      "        nan 0.50144841 0.49714314]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Participants for Training: [  0   1   3   4   5   6   7   8   9  10  12  14  15  16  17  18  20  21\n",
      "  22  23  24  25  26  27  29  31  32  33  34  35  36  37  38  39  40  42\n",
      "  43  44  45  46  48  49  50  51  52  53  55  56  57  58  59  61  62  64\n",
      "  65  66  67  68  69  70  71  72  74  75  76  77  79  80  81  82  83  84\n",
      "  87  88  89  90  92  96  98  99 100 101 102 103 104 105 108 109 110 111\n",
      " 113 114 115 118 120 121 123 124 125 126 127 128 129 132 133 134 135 136\n",
      " 137 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 173 174\n",
      " 175 176 177 179 180 181 183 184 185 186 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199 200 201 202 203 205 207 208 209 210 211 212 213 214 215\n",
      " 216 220 221 222 223 224 225 226 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 255 257\n",
      " 258 259 260 261 262 263 264 265 266 267 269 270 271 272 273 274 275 276\n",
      " 277 278 279 280 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
      " 296 297 298 299 300 301 303 305 306 308 309 311 314 315 316 317 318 319\n",
      " 320 321 322 323 325 326 327 328 329 330 331 334 336 337 338 339 340 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 357 359 360 361 362 364 365\n",
      " 366 367 368 369 370 372 373 374 376 378 380 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 403 404 405 406 407\n",
      " 410 412 413 415 416 417 419 420 421 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 436 438 439 440 441 442 445 446 447 448 450 452 453 454 455\n",
      " 456 457 458 460 461 462 463 464 465 466 467 468 469 471 472 473 474 475\n",
      " 476 477 478 479 480 481 482 483 484 485 486 487 489 490 491 492 493 494\n",
      " 495 496 497 498 499 500 501 502 505 506 508 509 510 511 512 513 514 515\n",
      " 516 517 518 520 521 522 523 524 525 526 527 530 531 532 533 534 535 537\n",
      " 538 539 540 541 543 544 545 546 547 548 549 550 551 552 553 556 557 558\n",
      " 559 562 565 567 568 569 570 571 572 574 575 576 577 578 579 580 582 583\n",
      " 584 585 586 588 591 592 593 594 595 597 598 599 600 601 602 603 604 606\n",
      " 608 609 610 611 614 615 616 618 619 620 621 622 623 624 625 626 627 628\n",
      " 629 630 631 632 633 635 636 637 638 639 640 641 642 643 644 645 647 648\n",
      " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
      " 667 669 670 672 674 675 676 677 678 679 680 681 684 685 686 687 688 689\n",
      " 691 692 693 695 696 697 698 699 700 701 702 703 704 705 707 708 709 711\n",
      " 712 715 716 717 719 720 721 722 723 724 725 726 728 729 730 732 733 734\n",
      " 735 737 738 740 741 742 743 744 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 767 768 769 770 772 775 776 778\n",
      " 779 780 781 783 784 785 786 787 788 789 790 791 794 795 796 797 798 799\n",
      " 800 801 802 803 804 806 808 809 810 811 812 814 815 816 817 818 820 821\n",
      " 822 823 824 825 826 827 828 829 830 832 833 834 835 836 837 838 840 841\n",
      " 842 843 844 846]\n",
      "Fold 1 - Participants for Validation: [  2  11  13  19  28  30  41  47  54  60  63  73  78  85  86  91  93  94\n",
      "  95  97 106 107 112 116 117 119 122 130 131 138 172 178 182 187 204 206\n",
      " 217 218 219 227 254 256 268 281 302 304 307 310 312 313 324 332 333 335\n",
      " 341 354 355 356 358 363 371 375 377 379 381 402 408 409 411 414 418 422\n",
      " 435 437 443 444 449 451 459 470 488 503 504 507 519 528 529 536 542 554\n",
      " 555 560 561 563 564 566 573 581 587 589 590 596 605 607 612 613 617 634\n",
      " 646 668 671 673 682 683 690 694 706 710 713 714 718 727 731 736 739 745\n",
      " 766 771 773 774 777 782 792 793 805 807 813 819 831 839 845 847]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.54520028        nan 0.52807911 0.52521227 0.52521227 0.52521227\n",
      "        nan 0.49291779 0.4885426  0.49004095        nan 0.49422635\n",
      " 0.49564479 0.48999101 0.48997103        nan 0.49291779 0.4885426\n",
      " 0.49433623        nan 0.49279792 0.48439716 0.49149935 0.49137948\n",
      "        nan 0.49291779 0.4885426 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Participants for Training: [  0   1   2   3   4   5   6  11  13  15  17  18  19  20  21  22  23  24\n",
      "  25  26  27  28  29  30  31  32  34  36  37  38  39  40  41  42  43  46\n",
      "  47  49  50  51  52  53  54  55  56  58  59  60  63  64  65  66  67  71\n",
      "  73  74  75  76  77  78  79  80  82  83  84  85  86  87  89  90  91  92\n",
      "  93  94  95  97  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 124 125 127 128 129 130 131 132\n",
      " 133 135 137 138 140 143 144 146 147 148 149 150 151 153 154 155 156 158\n",
      " 159 161 163 164 165 166 167 168 169 170 171 172 174 175 176 177 178 180\n",
      " 181 182 183 184 185 186 187 188 191 192 193 194 195 196 197 198 200 201\n",
      " 202 203 204 206 207 208 210 212 213 214 216 217 218 219 220 222 223 225\n",
      " 226 227 228 229 230 231 232 233 236 237 238 240 241 242 243 244 245 246\n",
      " 247 248 249 250 251 252 254 255 256 257 258 259 260 262 263 264 265 266\n",
      " 267 268 270 271 272 273 274 275 276 277 278 280 281 282 284 285 286 289\n",
      " 290 291 293 295 296 297 299 300 301 302 303 304 306 307 308 309 310 312\n",
      " 313 314 315 316 317 320 321 324 325 326 327 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 346 347 348 349 350 352 353 354 355\n",
      " 356 357 358 359 360 361 363 364 365 366 367 368 369 371 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 408 409 410 411 413 414 415\n",
      " 417 418 419 420 421 422 423 424 426 427 428 429 431 432 433 434 435 436\n",
      " 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 472 473 474\n",
      " 475 477 478 480 481 482 483 486 487 488 489 493 494 497 499 500 501 503\n",
      " 504 505 506 507 508 509 511 512 513 514 515 516 517 518 519 521 523 524\n",
      " 527 528 529 531 532 533 535 536 537 542 544 545 546 547 548 549 550 551\n",
      " 552 553 554 555 557 558 560 561 562 563 564 566 567 568 569 571 572 573\n",
      " 574 575 576 577 579 580 581 582 583 584 585 586 587 588 589 590 591 592\n",
      " 593 594 595 596 597 598 599 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 622 623 624 625 626 627 628 629 632\n",
      " 633 634 635 637 638 640 642 643 644 646 647 648 649 650 651 652 653 654\n",
      " 655 656 657 658 659 660 662 663 665 666 667 668 669 671 672 673 674 675\n",
      " 676 677 678 679 681 682 683 684 685 686 687 689 690 691 692 694 695 696\n",
      " 697 698 699 700 701 703 704 705 706 707 709 710 711 712 713 714 715 716\n",
      " 717 718 719 720 721 722 723 724 725 726 727 728 729 731 732 733 734 735\n",
      " 736 737 738 739 740 741 742 743 744 745 747 748 749 750 751 752 753 756\n",
      " 757 760 762 764 765 766 767 768 769 771 772 773 774 775 776 777 778 779\n",
      " 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798\n",
      " 799 800 801 802 803 804 805 806 807 808 809 810 811 813 814 815 816 817\n",
      " 819 820 821 822 823 825 826 828 829 830 831 832 833 834 835 836 837 839\n",
      " 843 844 845 847]\n",
      "Fold 2 - Participants for Validation: [  7   8   9  10  12  14  16  33  35  44  45  48  57  61  62  68  69  70\n",
      "  72  81  88  96  98 123 126 134 136 139 141 142 145 152 157 160 162 173\n",
      " 179 189 190 199 205 209 211 215 221 224 234 235 239 253 261 269 279 283\n",
      " 287 288 292 294 298 305 311 318 319 322 323 328 345 351 362 370 372 373\n",
      " 407 412 416 425 430 452 471 476 479 484 485 490 491 492 495 496 498 502\n",
      " 510 520 522 525 526 530 534 538 539 540 541 543 556 559 565 570 578 600\n",
      " 621 630 631 636 639 641 645 661 664 670 680 688 693 702 708 730 746 754\n",
      " 755 758 759 761 763 770 780 812 818 824 827 838 840 841 842 846]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.51765058        nan 0.51740086 0.46950355 0.47232045 0.470912\n",
      "        nan 0.48917191 0.48076116 0.49059035        nan 0.49764259\n",
      " 0.49624413 0.48774348 0.47935271        nan 0.48917191 0.48076116\n",
      " 0.48634502        nan 0.48076116 0.48069124 0.48494656 0.48076116\n",
      "        nan 0.48917191 0.48076116]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Participants for Training: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  22  25  26  27  28  29  30  31  32  33  34  35  38  40  41  43  44\n",
      "  45  46  47  48  49  51  52  53  54  55  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  71  72  73  74  76  77  78  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 103 106 107 108 109 110 111 112 113 114 115 116 117 118 119 122 123 124\n",
      " 125 126 127 128 129 130 131 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 147 148 151 152 157 160 162 163 164 165 166 167 168 169 170 171\n",
      " 172 173 174 176 177 178 179 180 182 183 184 187 188 189 190 191 192 194\n",
      " 195 196 197 198 199 200 204 205 206 207 209 210 211 213 214 215 216 217\n",
      " 218 219 221 222 224 227 228 229 230 231 232 233 234 235 236 238 239 240\n",
      " 241 242 243 244 245 246 248 249 250 251 252 253 254 255 256 257 258 260\n",
      " 261 264 265 266 267 268 269 270 272 273 274 275 276 277 278 279 281 282\n",
      " 283 285 286 287 288 289 290 291 292 293 294 295 297 298 299 300 301 302\n",
      " 303 304 305 306 307 310 311 312 313 314 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 331 332 333 334 335 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 351 352 353 354 355 356 357 358 359 361 362 363 364\n",
      " 365 366 367 369 370 371 372 373 374 375 376 377 378 379 381 382 383 385\n",
      " 386 387 388 389 390 393 394 395 396 397 398 399 402 403 405 406 407 408\n",
      " 409 410 411 412 414 415 416 417 418 419 420 421 422 423 424 425 426 428\n",
      " 429 430 431 432 433 434 435 436 437 440 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 457 458 459 461 462 463 464 465 466 467 468 470 471 474\n",
      " 475 476 477 479 480 481 482 483 484 485 486 487 488 490 491 492 493 494\n",
      " 495 496 497 498 499 501 502 503 504 505 506 507 508 510 511 513 514 515\n",
      " 516 517 518 519 520 522 523 524 525 526 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 545 546 547 548 549 550 551 552 553 554\n",
      " 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572\n",
      " 573 575 576 577 578 580 581 582 585 587 588 589 590 591 592 593 594 595\n",
      " 596 599 600 601 602 603 604 605 607 609 610 612 613 614 615 616 617 618\n",
      " 619 620 621 623 624 625 626 627 628 629 630 631 634 636 638 639 641 642\n",
      " 643 644 645 646 648 650 651 652 653 656 657 658 659 661 664 665 666 668\n",
      " 669 670 671 672 673 675 676 677 678 679 680 682 683 685 687 688 690 691\n",
      " 692 693 694 695 696 698 699 700 701 702 703 704 706 708 709 710 711 712\n",
      " 713 714 715 716 717 718 719 721 722 723 724 725 726 727 729 730 731 732\n",
      " 733 734 735 736 738 739 740 743 744 745 746 748 749 751 752 753 754 755\n",
      " 756 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
      " 775 777 778 779 780 782 786 788 789 790 791 792 793 794 795 796 798 799\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 817 818 819\n",
      " 820 821 822 824 827 828 829 830 831 834 835 836 837 838 839 840 841 842\n",
      " 843 844 845 846 847]\n",
      "Fold 3 - Participants for Validation: [  0   3  21  23  24  36  37  39  42  50  75  79 102 104 105 120 121 132\n",
      " 146 149 150 153 154 155 156 158 159 161 175 181 185 186 193 201 202 203\n",
      " 208 212 220 223 225 226 237 247 259 262 263 271 280 284 296 308 309 315\n",
      " 316 330 336 350 360 368 380 384 391 392 400 401 404 413 427 438 439 441\n",
      " 442 455 456 460 469 472 473 478 489 500 509 512 521 527 544 574 579 583\n",
      " 584 586 597 598 606 608 611 622 632 633 635 637 640 647 649 654 655 660\n",
      " 662 663 667 674 681 684 686 689 697 705 707 720 728 737 741 742 747 750\n",
      " 757 776 781 783 784 785 787 797 800 816 823 825 826 832 833]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.53868744        nan 0.53723904 0.53580062 0.53580062 0.53580062\n",
      "        nan 0.49488563 0.52873839 0.50194786        nan 0.53860753\n",
      " 0.52449306 0.48783338 0.53015683        nan 0.49488563 0.52873839\n",
      " 0.49065028        nan 0.53015683 0.4849965  0.48923184 0.53014684\n",
      "        nan 0.49488563 0.52873839]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Participants for Training: [  0   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  21  23  24  26  28  29  30  31  33  34  35  36  37  38  39  41  42  43\n",
      "  44  45  46  47  48  49  50  52  53  54  55  56  57  60  61  62  63  65\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 104 105\n",
      " 106 107 108 109 110 112 113 115 116 117 118 119 120 121 122 123 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 166 167 168 169 170 171 172 173 174 175 177 178 179 180 181 182 183 184\n",
      " 185 186 187 189 190 191 193 194 197 198 199 201 202 203 204 205 206 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 223 224 225 226 227\n",
      " 228 229 232 234 235 237 238 239 240 243 244 245 246 247 249 250 251 252\n",
      " 253 254 255 256 257 258 259 261 262 263 264 265 266 268 269 270 271 272\n",
      " 273 274 275 277 279 280 281 282 283 284 287 288 290 291 292 293 294 295\n",
      " 296 298 299 300 301 302 303 304 305 307 308 309 310 311 312 313 314 315\n",
      " 316 317 318 319 320 322 323 324 326 327 328 329 330 331 332 333 334 335\n",
      " 336 339 340 341 343 344 345 347 350 351 352 354 355 356 358 359 360 361\n",
      " 362 363 364 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383 384 386 388 389 391 392 393 394 396 397 398 400 401 402 404 405\n",
      " 407 408 409 410 411 412 413 414 416 417 418 419 422 423 424 425 427 428\n",
      " 429 430 431 432 433 435 436 437 438 439 440 441 442 443 444 445 447 449\n",
      " 450 451 452 453 454 455 456 459 460 462 464 465 466 468 469 470 471 472\n",
      " 473 474 475 476 477 478 479 480 481 484 485 486 487 488 489 490 491 492\n",
      " 494 495 496 497 498 499 500 501 502 503 504 506 507 508 509 510 512 514\n",
      " 516 518 519 520 521 522 523 525 526 527 528 529 530 531 532 533 534 536\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 552 553 554 555 556\n",
      " 557 559 560 561 563 564 565 566 567 568 569 570 573 574 575 577 578 579\n",
      " 580 581 582 583 584 586 587 588 589 590 591 593 596 597 598 599 600 601\n",
      " 603 604 605 606 607 608 611 612 613 614 616 617 618 619 620 621 622 623\n",
      " 625 626 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 649 650 651 652 653 654 655 656 657 659 660 661 662 663\n",
      " 664 665 666 667 668 669 670 671 672 673 674 676 677 678 680 681 682 683\n",
      " 684 685 686 687 688 689 690 692 693 694 695 696 697 700 701 702 703 704\n",
      " 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 723\n",
      " 725 726 727 728 729 730 731 733 734 736 737 738 739 741 742 743 744 745\n",
      " 746 747 749 750 754 755 756 757 758 759 760 761 763 764 766 769 770 771\n",
      " 772 773 774 775 776 777 778 780 781 782 783 784 785 786 787 789 790 792\n",
      " 793 794 796 797 798 800 801 802 803 805 807 808 811 812 813 814 816 818\n",
      " 819 820 822 823 824 825 826 827 829 831 832 833 835 836 838 839 840 841\n",
      " 842 843 845 846 847]\n",
      "Fold 4 - Participants for Validation: [  1   4  20  22  25  27  32  40  51  58  59  64  66  67  80 103 111 114\n",
      " 124 163 164 165 176 188 192 195 196 200 207 222 230 231 233 236 241 242\n",
      " 248 260 267 276 278 285 286 289 297 306 321 325 337 338 342 346 348 349\n",
      " 353 357 365 366 385 387 390 395 399 403 406 415 420 421 426 434 446 448\n",
      " 457 458 461 463 467 482 483 493 505 511 513 515 517 524 535 537 551 558\n",
      " 562 571 572 576 585 592 594 595 602 609 610 615 624 627 648 658 675 679\n",
      " 691 698 699 722 724 732 735 740 748 751 752 753 762 765 767 768 779 788\n",
      " 791 795 799 804 806 809 810 815 817 821 828 830 834 837 844]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.53032664        nan 0.524543   0.52025772 0.52025772 0.52025772\n",
      "        nan 0.4892718  0.46668664 0.46944361        nan 0.48504645\n",
      " 0.48218959 0.47935271 0.47232045        nan 0.4892718  0.46668664\n",
      " 0.47934272        nan 0.47233044 0.48357806 0.4849965  0.46951354\n",
      "        nan 0.4892718  0.46668664]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Participants for Training: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
      "  19  20  21  22  23  24  25  27  28  29  30  32  33  35  36  37  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59\n",
      "  60  61  62  63  64  66  67  68  69  70  71  72  73  74  75  76  78  79\n",
      "  80  81  84  85  86  88  89  90  91  92  93  94  95  96  97  98 100 102\n",
      " 103 104 105 106 107 109 110 111 112 113 114 116 117 119 120 121 122 123\n",
      " 124 125 126 129 130 131 132 133 134 136 137 138 139 140 141 142 144 145\n",
      " 146 147 149 150 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 169 172 173 175 176 178 179 181 182 183 185 186 187 188 189 190 191 192\n",
      " 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210\n",
      " 211 212 215 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 233 234 235 236 237 239 240 241 242 243 244 245 247 248 251 253 254 256\n",
      " 258 259 260 261 262 263 264 265 267 268 269 270 271 274 276 277 278 279\n",
      " 280 281 283 284 285 286 287 288 289 290 292 294 295 296 297 298 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 315 316 317 318 319 321 322\n",
      " 323 324 325 328 330 332 333 334 335 336 337 338 339 341 342 344 345 346\n",
      " 347 348 349 350 351 353 354 355 356 357 358 359 360 361 362 363 365 366\n",
      " 368 369 370 371 372 373 374 375 377 379 380 381 382 383 384 385 386 387\n",
      " 388 390 391 392 394 395 396 397 399 400 401 402 403 404 406 407 408 409\n",
      " 410 411 412 413 414 415 416 418 420 421 422 425 426 427 429 430 432 434\n",
      " 435 436 437 438 439 440 441 442 443 444 446 448 449 451 452 454 455 456\n",
      " 457 458 459 460 461 462 463 465 467 469 470 471 472 473 474 475 476 477\n",
      " 478 479 480 482 483 484 485 486 487 488 489 490 491 492 493 495 496 498\n",
      " 499 500 502 503 504 505 507 509 510 511 512 513 515 517 518 519 520 521\n",
      " 522 524 525 526 527 528 529 530 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 547 548 549 550 551 554 555 556 558 559 560 561 562 563 564\n",
      " 565 566 567 569 570 571 572 573 574 575 576 577 578 579 581 583 584 585\n",
      " 586 587 588 589 590 591 592 594 595 596 597 598 600 601 602 605 606 607\n",
      " 608 609 610 611 612 613 615 616 617 618 620 621 622 623 624 625 626 627\n",
      " 629 630 631 632 633 634 635 636 637 639 640 641 642 645 646 647 648 649\n",
      " 650 652 654 655 656 658 659 660 661 662 663 664 665 667 668 670 671 673\n",
      " 674 675 676 677 678 679 680 681 682 683 684 685 686 688 689 690 691 693\n",
      " 694 696 697 698 699 701 702 703 705 706 707 708 709 710 713 714 715 716\n",
      " 717 718 719 720 722 723 724 725 727 728 730 731 732 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 757 758\n",
      " 759 760 761 762 763 765 766 767 768 770 771 772 773 774 776 777 779 780\n",
      " 781 782 783 784 785 787 788 789 791 792 793 794 795 796 797 798 799 800\n",
      " 801 802 804 805 806 807 808 809 810 812 813 815 816 817 818 819 820 821\n",
      " 823 824 825 826 827 828 830 831 832 833 834 835 836 837 838 839 840 841\n",
      " 842 844 845 846 847]\n",
      "Fold 5 - Participants for Validation: [ 15  26  31  34  38  56  65  77  82  83  87  99 101 108 115 118 127 128\n",
      " 135 143 148 151 166 167 168 170 171 174 177 180 184 213 214 216 232 238\n",
      " 246 249 250 252 255 257 266 272 273 275 282 291 293 299 300 314 320 326\n",
      " 327 329 331 340 343 352 364 367 376 378 389 393 398 405 417 419 423 424\n",
      " 428 431 433 445 447 450 453 464 466 468 481 494 497 501 506 508 514 516\n",
      " 523 531 532 546 552 553 557 568 580 582 593 599 603 604 614 619 628 638\n",
      " 643 644 651 653 657 666 669 672 687 692 695 700 704 711 712 721 726 729\n",
      " 733 734 756 764 769 775 778 786 790 803 811 814 822 829 843]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.52169613        nan 0.5259115  0.47379882 0.47379882 0.47379882\n",
      "        nan 0.52599141 0.49352712 0.51322545        nan 0.48364799\n",
      " 0.50056937 0.52448307 0.49213865        nan 0.52599141 0.49352712\n",
      " 0.52457297        nan 0.49353711 0.52169613 0.52312456 0.49071022\n",
      "        nan 0.52599141 0.49352712]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 - Participants for Training: [  0   1   2   3   4   7   8   9  10  11  12  13  14  15  16  19  20  21\n",
      "  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  44  45  47  48  50  51  54  56  57  58  59  60  61  62  63  64\n",
      "  65  66  67  68  69  70  72  73  75  77  78  79  80  81  82  83  85  86\n",
      "  87  88  91  93  94  95  96  97  98  99 101 102 103 104 105 106 107 108\n",
      " 111 112 114 115 116 117 118 119 120 121 122 123 124 126 127 128 130 131\n",
      " 132 134 135 136 138 139 141 142 143 145 146 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 184 185 186 187 188 189 190 192 193\n",
      " 195 196 199 200 201 202 203 204 205 206 207 208 209 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 230 231 232 233 234 235\n",
      " 236 237 238 239 241 242 246 247 248 249 250 252 253 254 255 256 257 259\n",
      " 260 261 262 263 266 267 268 269 271 272 273 275 276 278 279 280 281 282\n",
      " 283 284 285 286 287 288 289 291 292 293 294 296 297 298 299 300 302 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 335 336 337 338 340 341 342 343\n",
      " 345 346 348 349 350 351 352 353 354 355 356 357 358 360 362 363 364 365\n",
      " 366 367 368 370 371 372 373 375 376 377 378 379 380 381 384 385 387 389\n",
      " 390 391 392 393 395 398 399 400 401 402 403 404 405 406 407 408 409 411\n",
      " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 430\n",
      " 431 433 434 435 437 438 439 441 442 443 444 445 446 447 448 449 450 451\n",
      " 452 453 455 456 457 458 459 460 461 463 464 466 467 468 469 470 471 472\n",
      " 473 476 478 479 481 482 483 484 485 488 489 490 491 492 493 494 495 496\n",
      " 497 498 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515\n",
      " 516 517 519 520 521 522 523 524 525 526 527 528 529 530 531 532 534 535\n",
      " 536 537 538 539 540 541 542 543 544 546 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 568 570 571 572 573 574 576 578 579 580\n",
      " 581 582 583 584 585 586 587 589 590 592 593 594 595 596 597 598 599 600\n",
      " 602 603 604 605 606 607 608 609 610 611 612 613 614 615 617 619 621 622\n",
      " 624 627 628 630 631 632 633 634 635 636 637 638 639 640 641 643 644 645\n",
      " 646 647 648 649 651 653 654 655 657 658 660 661 662 663 664 666 667 668\n",
      " 669 670 671 672 673 674 675 679 680 681 682 683 684 686 687 688 689 690\n",
      " 691 692 693 694 695 697 698 699 700 702 704 705 706 707 708 710 711 712\n",
      " 713 714 718 720 721 722 724 726 727 728 729 730 731 732 733 734 735 736\n",
      " 737 739 740 741 742 745 746 747 748 750 751 752 753 754 755 756 757 758\n",
      " 759 761 762 763 764 765 766 767 768 769 770 771 773 774 775 776 777 778\n",
      " 779 780 781 782 783 784 785 786 787 788 790 791 792 793 795 797 799 800\n",
      " 803 804 805 806 807 809 810 811 812 813 814 815 816 817 818 819 821 822\n",
      " 823 824 825 826 827 828 829 830 831 832 833 834 837 838 839 840 841 842\n",
      " 843 844 845 846 847]\n",
      "Fold 6 - Participants for Validation: [  5   6  17  18  29  43  46  49  52  53  55  71  74  76  84  89  90  92\n",
      " 100 109 110 113 125 129 133 137 140 144 147 169 183 191 194 197 198 210\n",
      " 228 229 240 243 244 245 251 258 264 265 270 274 277 290 295 301 303 317\n",
      " 334 339 344 347 359 361 369 374 382 383 386 388 394 396 397 410 429 432\n",
      " 436 440 454 462 465 474 475 477 480 486 487 499 518 533 545 547 548 549\n",
      " 550 567 569 575 577 588 591 601 616 618 620 623 625 626 629 642 650 652\n",
      " 656 659 665 676 677 678 685 696 701 703 709 715 716 717 719 723 725 738\n",
      " 743 744 749 760 772 789 794 796 798 801 802 808 820 835 836]\n",
      "\n",
      "Voting Classifier - Average Training Accuracy: 0.9497654642032395\n",
      "Voting Classifier - Average Validation Accuracy: 0.8655978423733893\n",
      "Voting Classifier - Training Accuracies for each fold: [0.9121813031161473, 0.9929178470254958, 0.8557284299858557, 0.9504950495049505, 0.9929278642149929, 0.9943422913719944]\n",
      "Voting Classifier - Validation Accuracies for each fold: [0.8028169014084507, 0.9014084507042254, 0.8014184397163121, 0.900709219858156, 0.8865248226950354, 0.900709219858156]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "# Define the target variable\n",
    "target_variable = 'Neuropsychiatric symptoms-new'\n",
    "\n",
    "# Define the hyperparameters for each classifier\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.3, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "params_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2', 'none'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=61)\n",
    "xgb_classifier = XGBClassifier(random_state=61)\n",
    "logreg_classifier = LogisticRegression(random_state=61)\n",
    "\n",
    "# GridSearchCV for each classifier\n",
    "grid_rf = GridSearchCV(rf_classifier, params_rf, cv=5, scoring='accuracy')\n",
    "grid_xgb = GridSearchCV(xgb_classifier, params_xgb, cv=5, scoring='accuracy')\n",
    "grid_logreg = GridSearchCV(logreg_classifier, params_logreg, cv=5, scoring='accuracy')\n",
    "\n",
    "# List of models\n",
    "models = [grid_rf, grid_xgb, grid_logreg]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Stratified K-Fold cross-validation loop\n",
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(df_resampled.drop(target_variable, axis=1), df_resampled[target_variable])):\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_data = df_resampled.iloc[train_indices]\n",
    "    val_data = df_resampled.iloc[val_indices]\n",
    "\n",
    "    # Split data into features and labels for training set\n",
    "    X_train = train_data.drop(target_variable, axis=1)\n",
    "    y_train = train_data[target_variable]\n",
    "\n",
    "    # Split data into features and labels for validation set\n",
    "    X_val = val_data.drop(target_variable, axis=1)\n",
    "    y_val = val_data[target_variable]\n",
    "\n",
    "    # Fit the grid search models\n",
    "    grid_rf.fit(X_train, y_train)\n",
    "    grid_xgb.fit(X_train, y_train)\n",
    "    grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best estimators obtained from grid search\n",
    "    best_rf = grid_rf.best_estimator_\n",
    "    best_xgb = grid_xgb.best_estimator_\n",
    "    best_logreg = grid_logreg.best_estimator_\n",
    "\n",
    "    # Create a VotingClassifier with the best estimators\n",
    "    best_voting_classifier = VotingClassifier(estimators=[\n",
    "        ('random_forest', best_rf),\n",
    "        ('xgboost', best_xgb),\n",
    "        ('logistic_regression', best_logreg)\n",
    "    ], voting='hard')\n",
    "\n",
    "    # Fit the VotingClassifier with the best estimators\n",
    "    best_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the best VotingClassifier\n",
    "    train_preds = best_voting_classifier.predict(X_train)\n",
    "    val_preds = best_voting_classifier.predict(X_val)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    train_accuracies['Voting Classifier'].append(train_accuracy)\n",
    "    val_accuracies['Voting Classifier'].append(val_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Participants for Training: {train_indices}\")\n",
    "    print(f\"Fold {fold + 1} - Participants for Validation: {val_indices}\")\n",
    "    print()\n",
    "\n",
    "# Print or analyze the results for the Voting Classifier\n",
    "print(\"Voting Classifier - Average Training Accuracy:\", sum(train_accuracies['Voting Classifier']) / len(train_accuracies['Voting Classifier']))\n",
    "print(\"Voting Classifier - Average Validation Accuracy:\", sum(val_accuracies['Voting Classifier']) / len(val_accuracies['Voting Classifier']))\n",
    "print(\"Voting Classifier - Training Accuracies for each fold:\", train_accuracies['Voting Classifier'])\n",
    "print(\"Voting Classifier - Validation Accuracies for each fold:\", val_accuracies['Voting Classifier'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af7a17b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, random_state=61)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=200, random_state=61)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=61)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd3df28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=61, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=61, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=61, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ad4cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, penalty=&#x27;none&#x27;, random_state=61)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, penalty=&#x27;none&#x27;, random_state=61)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, penalty='none', random_state=61)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a83f2f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Participants for Training: [  0   1   2   3   5   6   7   9  10  11  12  13  14  15  16  18  19  20\n",
      "  21  22  23  24  28  29  30  31  32  33  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  62  64  65  66  67  68  70  71  72  74  76  77  79  80  83  84  85\n",
      "  86  87  89  90  92  94  96  98 102 103 104 105 107 108 111 112 113 115\n",
      " 116 117 118 119 120 121 122 124 125 126 127 128 129 130 131 132 133 134\n",
      " 136 138 139 140 141 142 143 144 145 146 147 148 150 151 152 154 155 156\n",
      " 157 159 160 163 164 165 166 169 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 203 204 206 207 208 209 210 211 212 213 215 216 217 218 219 220 221\n",
      " 222 223 225 226 227 228 229 230 232 233 234 235 236 237 238 239 240 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 256 257 259 260 261 262\n",
      " 263 264 265 266 268 269 270 271 272 275 277 278 279 280 281 282 285 286\n",
      " 287 289 290 291 292 293 294 295 297 298 300 301 302 303 304 305 307 311\n",
      " 312 314 315 317 318 319 320 321 322 323 324 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 341 342 343 344 345 346 347 348 349 350 351\n",
      " 352 353 354 355 356 357 358 360 361 363 365 366 367 368 369 370 371 372\n",
      " 373 374 375 376 377 378 380 381 382 383 384 385 386 387 388 389 390 391\n",
      " 392 394 395 396 397 399 400 401 402 403 404 405 406 407 408 409 410 411\n",
      " 413 414 415 416 417 418 419 421 422 423 425 426 427 428 429 430 431 432\n",
      " 433 434 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 460 461 463 464 465 466 468 469 470 471 472 473\n",
      " 474 476 477 478 480 481 482 483 484 487 488 489 490 492 493 494 495 497\n",
      " 498 499 500 501 502 503 504 505 506 507 508 509 511 512 513 514 515 516\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 532 533 534 535 536\n",
      " 537 538 539 540 541 542 543 544 545 547 548 550 551 552 553 554 555 556\n",
      " 557 558 559 560 561 562 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 578 579 580 581 582 584 585 587 588 590 591 592 593 594 595 596 597 598\n",
      " 599 600 601 602 604 605 606 607 608 609 610 611 612 613 615 616 618 619\n",
      " 620 622 623 624 625 626 627 630 631 632 634 635 636 637 638 639 641 642\n",
      " 643 646 647 648 649 650 651 652 655 656 657 658 659 660 662 663 664 665\n",
      " 668 669 670 671 672 673 674 675 676 678 679 680 682 683 684 685 686 687\n",
      " 688 689 690 691 693 694 695 697 698 701 702 703 704 705 706 708 709 710\n",
      " 711 712 713 714 715 716 717 718 719 723 725 726 727 728 729 730 731 733\n",
      " 734 736 737 738 739 741 742 743 744 745 746 747 749 750 751 752 753 754\n",
      " 755 756 759 760 761 762 764 765 766 767 768 769 770 771 772 774 775 777\n",
      " 778 779 780 781 782 783 784 785 787 789 790 791 793 794 795 796 797 798\n",
      " 799 800 801 802 803 806 807 808 809 811 814 815 816 818 819 821 823 824\n",
      " 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842\n",
      " 843 845 846 847]\n",
      "Fold 1 - Participants for Validation: [  4   8  17  25  26  27  61  63  69  73  75  78  81  82  88  91  93  95\n",
      "  97  99 100 101 106 109 110 114 123 135 137 149 153 158 161 162 167 168\n",
      " 170 187 188 202 205 214 224 231 241 255 258 267 273 274 276 283 284 288\n",
      " 296 299 306 308 309 310 313 316 325 340 359 362 364 379 393 398 412 420\n",
      " 424 435 436 459 462 467 475 479 485 486 491 496 510 517 531 546 549 563\n",
      " 576 577 583 586 589 603 614 617 621 628 629 633 640 644 645 653 654 661\n",
      " 666 667 677 681 692 696 699 700 707 720 721 722 724 732 735 740 748 757\n",
      " 758 763 773 776 786 788 792 804 805 810 812 813 817 820 822 844]\n",
      "\n",
      "Fold 2 - Participants for Training: [  0   2   3   4   5   6   8   9  10  11  12  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  32  33  34  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  54  55  56  58  59  60  61\n",
      "  62  63  64  65  68  69  70  71  72  73  74  75  77  78  79  80  81  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
      " 120 122 123 125 126 127 128 129 130 131 132 134 135 136 137 138 140 142\n",
      " 143 144 145 147 148 149 150 151 152 153 154 157 158 159 160 161 162 163\n",
      " 165 167 168 169 170 171 174 175 176 178 179 180 181 183 185 186 187 188\n",
      " 189 190 192 193 194 195 196 197 199 200 201 202 204 205 206 207 209 210\n",
      " 211 212 213 214 215 217 218 219 220 222 223 224 225 226 227 228 229 231\n",
      " 232 233 234 235 236 238 240 241 243 244 246 247 248 249 250 252 253 254\n",
      " 255 256 257 258 259 261 262 263 265 266 267 268 269 270 273 274 275 276\n",
      " 277 278 279 280 281 282 283 284 286 287 288 290 291 292 293 295 296 299\n",
      " 300 301 302 303 304 306 307 308 309 310 312 313 315 316 317 318 319 321\n",
      " 322 323 325 326 327 328 330 331 333 334 335 337 340 341 342 343 344 346\n",
      " 347 349 350 351 352 354 356 357 358 359 360 361 362 364 365 366 367 370\n",
      " 372 373 374 375 376 377 378 379 380 383 384 385 386 387 388 389 390 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 406 407 409 410 411 412 413\n",
      " 414 415 416 418 419 420 421 422 424 426 427 428 429 432 433 434 435 436\n",
      " 437 438 439 440 442 443 444 445 446 447 448 449 450 451 453 454 455 456\n",
      " 457 458 459 461 462 463 464 466 467 468 469 470 471 472 473 474 475 476\n",
      " 478 479 481 482 484 485 486 487 488 489 490 491 492 493 494 495 496 497\n",
      " 498 499 500 501 503 504 505 506 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 527 528 530 531 533 534 535 536 539 541 543\n",
      " 544 545 546 547 549 550 551 553 554 555 556 557 558 560 561 562 563 564\n",
      " 565 566 567 568 569 570 571 572 573 575 576 577 580 582 583 584 585 586\n",
      " 587 588 589 591 592 593 595 596 598 599 600 601 602 603 604 605 606 607\n",
      " 608 609 610 611 612 613 614 617 618 619 620 621 624 625 626 627 628 629\n",
      " 631 632 633 634 636 638 640 642 644 645 646 647 648 651 652 653 654 655\n",
      " 656 657 658 660 661 663 664 665 666 667 668 669 671 673 674 676 677 678\n",
      " 679 680 681 682 683 684 686 687 688 689 691 692 693 694 695 696 697 698\n",
      " 699 700 701 702 704 705 706 707 708 710 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 732 733 734 735 736 737 738\n",
      " 739 740 741 742 743 745 746 747 748 749 750 751 752 753 755 756 757 758\n",
      " 760 763 765 767 768 769 770 772 773 774 775 776 778 779 780 781 783 784\n",
      " 786 787 788 789 790 791 792 793 794 796 797 798 799 800 802 803 804 805\n",
      " 806 807 808 809 810 812 813 814 815 816 817 818 819 820 821 822 823 824\n",
      " 825 826 827 828 829 830 831 833 834 835 836 837 838 839 840 841 842 843\n",
      " 844 845 846 847]\n",
      "Fold 2 - Participants for Validation: [  1   7  13  31  35  36  53  57  66  67  76 112 121 124 133 139 141 146\n",
      " 155 156 164 166 172 173 177 182 184 191 198 203 208 216 221 230 237 239\n",
      " 242 245 251 260 264 271 272 285 289 294 297 298 305 311 314 320 324 329\n",
      " 332 336 338 339 345 348 353 355 363 368 369 371 381 382 391 404 405 408\n",
      " 417 423 425 430 431 441 452 460 465 477 480 483 502 507 525 526 529 532\n",
      " 537 538 540 542 548 552 559 574 578 579 581 590 594 597 615 616 622 623\n",
      " 630 635 637 639 641 643 649 650 659 662 670 672 675 685 690 703 709 711\n",
      " 731 744 754 759 761 762 764 766 771 777 782 785 795 801 811 832]\n",
      "\n",
      "Fold 3 - Participants for Training: [  0   1   2   3   4   6   7   8  10  12  13  14  15  16  17  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  38  40  41\n",
      "  42  43  44  47  48  49  51  53  54  55  56  57  59  60  61  62  63  64\n",
      "  66  67  68  69  71  72  73  74  75  76  77  78  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  99 100 101 102 103 104\n",
      " 105 106 108 109 110 111 112 114 115 118 119 120 121 122 123 124 125 126\n",
      " 129 131 132 133 134 135 136 137 138 139 140 141 143 144 145 146 147 149\n",
      " 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 166 167 168\n",
      " 169 170 171 172 173 175 176 177 178 179 181 182 184 185 186 187 188 190\n",
      " 191 192 193 194 195 196 197 198 199 200 202 203 204 205 206 207 208 210\n",
      " 211 213 214 215 216 217 218 220 221 222 224 225 227 229 230 231 232 234\n",
      " 235 236 237 238 239 241 242 243 244 245 246 247 251 252 253 254 255 257\n",
      " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 276\n",
      " 277 278 280 282 283 284 285 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 303 304 305 306 308 309 310 311 312 313 314 315 316 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 334 335 336 338 339 340 341\n",
      " 342 343 344 345 348 349 350 351 353 354 355 357 358 359 362 363 364 365\n",
      " 368 369 370 371 372 374 375 378 379 380 381 382 383 385 386 387 388 390\n",
      " 391 393 394 395 396 397 398 399 400 401 402 403 404 405 407 408 409 410\n",
      " 412 413 414 416 417 418 419 420 421 422 423 424 425 426 428 429 430 431\n",
      " 433 434 435 436 437 439 440 441 442 443 445 446 448 449 451 452 453 454\n",
      " 455 457 458 459 460 461 462 464 465 467 469 470 472 473 475 477 478 479\n",
      " 480 481 482 483 484 485 486 487 488 489 490 491 493 495 496 498 500 501\n",
      " 502 503 504 505 506 507 508 509 510 511 512 513 514 516 517 518 520 521\n",
      " 523 524 525 526 528 529 530 531 532 533 536 537 538 539 540 541 542 544\n",
      " 545 546 548 549 550 552 554 556 558 559 560 561 562 563 565 566 567 568\n",
      " 569 571 572 573 574 576 577 578 579 580 581 582 583 585 586 587 589 590\n",
      " 591 592 594 596 597 598 599 600 603 604 605 606 607 608 609 610 612 613\n",
      " 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631\n",
      " 632 633 635 637 638 639 640 641 643 644 645 646 647 648 649 650 651 652\n",
      " 653 654 655 656 658 659 660 661 662 663 666 667 668 669 670 672 673 674\n",
      " 675 676 677 678 679 680 681 682 683 684 685 687 688 690 692 693 694 696\n",
      " 697 699 700 701 702 703 705 706 707 708 709 710 711 712 713 714 715 716\n",
      " 717 718 719 720 721 722 723 724 725 727 728 730 731 732 734 735 736 737\n",
      " 740 741 742 743 744 745 746 747 748 749 750 752 753 754 755 757 758 759\n",
      " 760 761 762 763 764 765 766 767 768 769 770 771 773 774 775 776 777 778\n",
      " 779 780 782 783 784 785 786 788 790 791 792 793 794 795 796 798 799 800\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 817 818 819 820\n",
      " 822 823 824 825 827 829 830 831 832 833 834 835 837 838 839 840 841 842\n",
      " 843 844 845 846 847]\n",
      "Fold 3 - Participants for Validation: [  5   9  11  18  32  39  45  46  50  52  58  65  70  79  98 107 113 116\n",
      " 117 127 128 130 142 148 165 174 180 183 189 201 209 212 219 223 226 228\n",
      " 233 240 248 249 250 256 275 279 281 286 287 301 302 307 317 318 333 337\n",
      " 346 347 352 356 360 361 366 367 373 376 377 384 389 392 406 411 415 427\n",
      " 432 438 444 447 450 456 463 466 468 471 474 476 492 494 497 499 515 519\n",
      " 522 527 534 535 543 547 551 553 555 557 564 570 575 584 588 593 595 601\n",
      " 602 611 634 636 642 657 664 665 671 686 689 691 695 698 704 726 729 733\n",
      " 738 739 751 756 772 781 787 789 797 815 816 821 826 828 836]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Participants for Training: [  0   1   3   4   5   7   8   9  11  12  13  14  15  16  17  18  23  24\n",
      "  25  26  27  28  30  31  32  34  35  36  37  39  40  41  45  46  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66  67  68  69\n",
      "  70  71  73  75  76  77  78  79  80  81  82  83  84  85  86  87  88  91\n",
      "  93  95  96  97  98  99 100 101 102 103 104 106 107 108 109 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 133 134 135 136 137 138 139 141 142 146 148 149 150 151 152 153 155 156\n",
      " 158 159 160 161 162 163 164 165 166 167 168 170 172 173 174 175 177 178\n",
      " 180 182 183 184 185 187 188 189 190 191 192 193 196 197 198 200 201 202\n",
      " 203 205 206 207 208 209 211 212 214 216 217 218 219 220 221 222 223 224\n",
      " 225 226 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 264\n",
      " 265 266 267 268 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
      " 284 285 286 287 288 289 290 291 292 293 294 296 297 298 299 301 302 303\n",
      " 304 305 306 307 308 309 310 311 313 314 315 316 317 318 319 320 322 323\n",
      " 324 325 326 328 329 330 331 332 333 335 336 337 338 339 340 342 343 344\n",
      " 345 346 347 348 349 351 352 353 355 356 358 359 360 361 362 363 364 366\n",
      " 367 368 369 371 372 373 375 376 377 378 379 381 382 383 384 385 386 387\n",
      " 388 389 390 391 392 393 396 398 399 400 401 402 404 405 406 407 408 409\n",
      " 410 411 412 414 415 417 418 419 420 421 422 423 424 425 426 427 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 447 449 450\n",
      " 451 452 453 454 455 456 459 460 462 463 464 465 466 467 468 469 471 474\n",
      " 475 476 477 479 480 481 483 484 485 486 487 488 490 491 492 493 494 495\n",
      " 496 497 499 500 501 502 504 505 506 507 508 509 510 511 512 514 515 517\n",
      " 518 519 520 522 523 524 525 526 527 528 529 530 531 532 534 535 536 537\n",
      " 538 539 540 541 542 543 545 546 547 548 549 550 551 552 553 555 557 558\n",
      " 559 560 561 562 563 564 569 570 571 572 573 574 575 576 577 578 579 580\n",
      " 581 582 583 584 585 586 588 589 590 593 594 595 597 599 600 601 602 603\n",
      " 604 606 607 608 609 611 612 613 614 615 616 617 619 620 621 622 623 624\n",
      " 625 626 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 649 650 651 653 654 657 658 659 660 661 662 664 665 666\n",
      " 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 683 684 685\n",
      " 686 687 688 689 690 691 692 693 695 696 697 698 699 700 701 702 703 704\n",
      " 706 707 708 709 711 712 717 718 719 720 721 722 723 724 726 727 728 729\n",
      " 731 732 733 734 735 737 738 739 740 742 744 746 748 749 750 751 753 754\n",
      " 755 756 757 758 759 760 761 762 763 764 766 767 768 771 772 773 775 776\n",
      " 777 778 780 781 782 783 785 786 787 788 789 792 794 795 797 798 799 800\n",
      " 801 803 804 805 806 808 809 810 811 812 813 814 815 816 817 818 820 821\n",
      " 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839\n",
      " 840 842 843 844 847]\n",
      "Fold 4 - Participants for Validation: [  2   6  10  19  20  21  22  29  33  38  42  43  44  47  48  64  72  74\n",
      "  89  90  92  94 105 131 132 140 143 144 145 147 154 157 169 171 176 179\n",
      " 181 186 194 195 199 204 210 213 215 227 246 262 263 269 295 300 312 321\n",
      " 327 334 341 350 354 357 365 370 374 380 394 395 397 403 413 416 428 446\n",
      " 448 457 458 461 470 472 473 478 482 489 498 503 513 516 521 533 544 554\n",
      " 556 565 566 567 568 587 591 592 596 598 605 610 618 627 648 652 655 656\n",
      " 663 682 694 705 710 713 714 715 716 725 730 736 741 743 745 747 752 765\n",
      " 769 770 774 779 784 790 791 793 796 802 807 819 841 845 846]\n",
      "\n",
      "Fold 5 - Participants for Training: [  0   1   2   3   4   5   6   7   8   9  10  11  13  15  17  18  19  20\n",
      "  21  22  23  25  26  27  28  29  30  31  32  33  35  36  38  39  41  42\n",
      "  43  44  45  46  47  48  50  52  53  54  55  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  72  73  74  75  76  77  78  79  80  81  82\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99 100 101 102\n",
      " 105 106 107 109 110 111 112 113 114 115 116 117 120 121 123 124 127 128\n",
      " 129 130 131 132 133 134 135 137 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 153 154 155 156 157 158 159 160 161 162 164 165 166 167 168 169\n",
      " 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 186 187 188\n",
      " 189 190 191 192 194 195 197 198 199 200 201 202 203 204 205 206 207 208\n",
      " 209 210 212 213 214 215 216 217 218 219 221 222 223 224 226 227 228 229\n",
      " 230 231 233 235 237 239 240 241 242 244 245 246 247 248 249 250 251 252\n",
      " 253 255 256 258 259 260 261 262 263 264 266 267 269 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 291 294 295 296\n",
      " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 316 317 318 319 320 321 322 324 325 327 328 329 331 332 333 334 335 336\n",
      " 337 338 339 340 341 342 344 345 346 347 348 350 352 353 354 355 356 357\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 376 377\n",
      " 379 380 381 382 383 384 387 389 391 392 393 394 395 397 398 402 403 404\n",
      " 405 406 407 408 411 412 413 415 416 417 419 420 422 423 424 425 427 428\n",
      " 429 430 431 432 433 435 436 437 438 440 441 444 445 446 447 448 450 452\n",
      " 454 456 457 458 459 460 461 462 463 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 485 486 487 489 491 492 494 495\n",
      " 496 497 498 499 501 502 503 504 507 509 510 512 513 515 516 517 518 519\n",
      " 520 521 522 525 526 527 528 529 530 531 532 533 534 535 537 538 540 542\n",
      " 543 544 545 546 547 548 549 551 552 553 554 555 556 557 558 559 560 563\n",
      " 564 565 566 567 568 569 570 571 572 574 575 576 577 578 579 581 582 583\n",
      " 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 601 602\n",
      " 603 605 606 607 610 611 613 614 615 616 617 618 619 620 621 622 623 624\n",
      " 625 626 627 628 629 630 631 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 647 648 649 650 652 653 654 655 656 657 659 660 661 662 663 664\n",
      " 665 666 667 668 670 671 672 675 676 677 681 682 683 684 685 686 689 690\n",
      " 691 692 694 695 696 697 698 699 700 701 703 704 705 706 707 709 710 711\n",
      " 712 713 714 715 716 717 720 721 722 724 725 726 727 728 729 730 731 732\n",
      " 733 734 735 736 738 739 740 741 743 744 745 746 747 748 750 751 752 754\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 769 770 771 772 773 774\n",
      " 776 777 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794\n",
      " 795 796 797 798 799 800 801 802 804 805 806 807 810 811 812 813 815 816\n",
      " 817 818 819 820 821 822 823 824 826 827 828 829 832 836 838 839 840 841\n",
      " 843 844 845 846 847]\n",
      "Fold 5 - Participants for Validation: [ 12  14  16  24  34  37  40  49  51  56  71  83  96 103 104 108 118 119\n",
      " 122 125 126 136 138 151 152 163 185 193 196 211 220 225 232 234 236 238\n",
      " 243 254 257 265 268 270 290 292 293 315 323 326 330 343 349 351 358 375\n",
      " 378 385 386 388 390 396 399 400 401 409 410 414 418 421 426 434 439 442\n",
      " 443 449 451 453 455 464 484 488 490 493 500 505 506 508 511 514 523 524\n",
      " 536 539 541 550 561 562 573 580 600 604 608 609 612 632 646 651 658 669\n",
      " 673 674 678 679 680 687 688 693 702 708 718 719 723 737 742 749 753 755\n",
      " 768 775 778 803 808 809 814 825 830 831 833 834 835 837 842]\n",
      "\n",
      "Fold 6 - Participants for Training: [  1   2   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19  20\n",
      "  21  22  24  25  26  27  29  31  32  33  34  35  36  37  38  39  40  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  56  57  58  61  63  64  65\n",
      "  66  67  69  70  71  72  73  74  75  76  78  79  81  82  83  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 103 104 105 106 107 108 109\n",
      " 110 112 113 114 116 117 118 119 121 122 123 124 125 126 127 128 130 131\n",
      " 132 133 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 151\n",
      " 152 153 154 155 156 157 158 161 162 163 164 165 166 167 168 169 170 171\n",
      " 172 173 174 176 177 179 180 181 182 183 184 185 186 187 188 189 191 193\n",
      " 194 195 196 198 199 201 202 203 204 205 208 209 210 211 212 213 214 215\n",
      " 216 219 220 221 223 224 225 226 227 228 230 231 232 233 234 236 237 238\n",
      " 239 240 241 242 243 245 246 248 249 250 251 254 255 256 257 258 260 262\n",
      " 263 264 265 267 268 269 270 271 272 273 274 275 276 279 281 283 284 285\n",
      " 286 287 288 289 290 292 293 294 295 296 297 298 299 300 301 302 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 320 321 323 324 325 326\n",
      " 327 329 330 332 333 334 336 337 338 339 340 341 343 345 346 347 348 349\n",
      " 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 373 374 375 376 377 378 379 380 381 382 384 385 386 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 403 404 405 406 408\n",
      " 409 410 411 412 413 414 415 416 417 418 420 421 423 424 425 426 427 428\n",
      " 430 431 432 434 435 436 438 439 441 442 443 444 446 447 448 449 450 451\n",
      " 452 453 455 456 457 458 459 460 461 462 463 464 465 466 467 468 470 471\n",
      " 472 473 474 475 476 477 478 479 480 482 483 484 485 486 488 489 490 491\n",
      " 492 493 494 496 497 498 499 500 502 503 505 506 507 508 510 511 513 514\n",
      " 515 516 517 519 521 522 523 524 525 526 527 529 531 532 533 534 535 536\n",
      " 537 538 539 540 541 542 543 544 546 547 548 549 550 551 552 553 554 555\n",
      " 556 557 559 561 562 563 564 565 566 567 568 570 573 574 575 576 577 578\n",
      " 579 580 581 583 584 586 587 588 589 590 591 592 593 594 595 596 597 598\n",
      " 600 601 602 603 604 605 608 609 610 611 612 614 615 616 617 618 621 622\n",
      " 623 627 628 629 630 632 633 634 635 636 637 639 640 641 642 643 644 645\n",
      " 646 648 649 650 651 652 653 654 655 656 657 658 659 661 662 663 664 665\n",
      " 666 667 669 670 671 672 673 674 675 677 678 679 680 681 682 685 686 687\n",
      " 688 689 690 691 692 693 694 695 696 698 699 700 702 703 704 705 707 708\n",
      " 709 710 711 713 714 715 716 718 719 720 721 722 723 724 725 726 729 730\n",
      " 731 732 733 735 736 737 738 739 740 741 742 743 744 745 747 748 749 751\n",
      " 752 753 754 755 756 757 758 759 761 762 763 764 765 766 768 769 770 771\n",
      " 772 773 774 775 776 777 778 779 781 782 784 785 786 787 788 789 790 791\n",
      " 792 793 795 796 797 801 802 803 804 805 807 808 809 810 811 812 813 814\n",
      " 815 816 817 819 820 821 822 825 826 828 830 831 832 833 834 835 836 837\n",
      " 841 842 844 845 846]\n",
      "Fold 6 - Participants for Validation: [  0   3  15  23  28  30  41  54  55  59  60  62  68  77  80  84  85  86\n",
      "  87 102 111 115 120 129 134 150 159 160 175 178 190 192 197 200 206 207\n",
      " 217 218 222 229 235 244 247 252 253 259 261 266 277 278 280 282 291 303\n",
      " 304 319 322 328 331 335 342 344 372 383 387 402 407 419 422 429 433 437\n",
      " 440 445 454 469 481 487 495 501 504 509 512 518 520 528 530 545 558 560\n",
      " 569 571 572 582 585 599 606 607 613 619 620 624 625 626 631 638 647 660\n",
      " 668 676 683 684 697 701 706 712 717 727 728 734 746 750 760 767 780 783\n",
      " 794 798 799 800 806 818 823 824 827 829 838 839 840 843 847]\n",
      "\n",
      "Random Forest - Average Training Accuracy: 0.9945753579809621\n",
      "Random Forest - Average Validation Accuracy: 0.9068524622914794\n",
      "Random Forest - Training Accuracies for each fold: [0.9957507082152974, 0.9929178470254958, 0.9943422913719944, 0.9957567185289957, 0.9943422913719944, 0.9943422913719944]\n",
      "Random Forest - Validation Accuracies for each fold: [0.8802816901408451, 0.9225352112676056, 0.9148936170212766, 0.9078014184397163, 0.9574468085106383, 0.8581560283687943]\n",
      "\n",
      "XGBoost - Average Training Accuracy: 0.994103548355645\n",
      "XGBoost - Average Validation Accuracy: 0.9375104052209235\n",
      "XGBoost - Training Accuracies for each fold: [0.9957507082152974, 0.9915014164305949, 0.9943422913719944, 0.9943422913719944, 0.9943422913719944, 0.9943422913719944]\n",
      "XGBoost - Validation Accuracies for each fold: [0.9436619718309859, 0.9225352112676056, 0.9432624113475178, 0.9219858156028369, 0.950354609929078, 0.9432624113475178]\n",
      "\n",
      "Logistic Regression - Average Training Accuracy: 0.5990569817807357\n",
      "Logistic Regression - Average Validation Accuracy: 0.5649369027403189\n",
      "Logistic Regression - Training Accuracies for each fold: [0.5934844192634561, 0.6062322946175638, 0.5785007072135785, 0.5983026874115983, 0.611032531824611, 0.6067892503536068]\n",
      "Logistic Regression - Validation Accuracies for each fold: [0.5633802816901409, 0.5, 0.5602836879432624, 0.5815602836879432, 0.6170212765957447, 0.5673758865248227]\n",
      "\n",
      "Voting Classifier - Average Training Accuracy: 0.994103548355645\n",
      "Voting Classifier - Average Validation Accuracy: 0.9280541404455099\n",
      "Voting Classifier - Training Accuracies for each fold: [0.9957507082152974, 0.9915014164305949, 0.9943422913719944, 0.9943422913719944, 0.9943422913719944, 0.9943422913719944]\n",
      "Voting Classifier - Validation Accuracies for each fold: [0.9366197183098591, 0.9295774647887324, 0.9361702127659575, 0.9078014184397163, 0.9432624113475178, 0.9148936170212766]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "# Define the target variable\n",
    "target_variable = 'Neuropsychiatric symptoms-new'\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=25)\n",
    "xgb_classifier = XGBClassifier(n_estimators=300, learning_rate=0.3,  random_state=25)\n",
    "logreg_classifier = LogisticRegression(C=0.1, penalty='none', random_state=25)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Lists to store training and validation accuracies for each model\n",
    "train_accuracies = {name: [] for name in names}\n",
    "val_accuracies = {name: [] for name in names}\n",
    "\n",
    "# Stratified K-Fold cross-validation loop\n",
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=27)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(df_resampled.drop(target_variable, axis=1), df_resampled[target_variable])):\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data = df_resampled.iloc[train_indices]\n",
    "    val_data = df_resampled.iloc[val_indices]\n",
    "\n",
    "    # Split data into features and labels for training set\n",
    "    X_train = train_data.drop(target_variable, axis=1)\n",
    "    y_train = train_data[target_variable]\n",
    "\n",
    "    # Split data into features and labels for validation set\n",
    "    X_val = val_data.drop(target_variable, axis=1)\n",
    "    y_val = val_data[target_variable]\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model, name in zip(models, names):\n",
    "        model.fit(X_train, y_train)\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "\n",
    "        train_accuracies[name].append(train_accuracy)\n",
    "        val_accuracies[name].append(val_accuracy)\n",
    "        \n",
    "    print(f\"Fold {fold + 1} - Participants for Training: {train_indices}\")\n",
    "    print(f\"Fold {fold + 1} - Participants for Validation: {val_indices}\")\n",
    "    print()\n",
    "\n",
    "# Print or analyze the results for each model\n",
    "for name in names:\n",
    "    print(f\"{name} - Average Training Accuracy: {sum(train_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Average Validation Accuracy: {sum(val_accuracies[name]) / num_folds}\")\n",
    "    print(f\"{name} - Training Accuracies for each fold: {train_accuracies[name]}\")\n",
    "    print(f\"{name} - Validation Accuracies for each fold: {val_accuracies[name]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf3fb984",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 3\u001b[0m df_train_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df_train_tmp \u001b[38;5;241m=\u001b[39m df_train_tmp\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Separate features (X) and target variable (y) for training dataset\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_train_tmp = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train.csv')\n",
    "df_train_tmp = df_train_tmp.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_tmp = df_train_tmp.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train_tmp['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_test = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_test.csv')\n",
    "df_test = df_test.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_test = df_test.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_test = df_test['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train_tmp = scaler.fit_transform(X_train_tmp)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da1aceaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, names):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 42\u001b[0m     y_pred_test_set \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n\u001b[0;32m     43\u001b[0m     test_set_score \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_test_set)\n\u001b[0;32m     44\u001b[0m     model_accuracy_test_set\u001b[38;5;241m.\u001b[39mappend(test_set_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your data from a CSV file\n",
    "df_resampled_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_byparticipants_new.csv')\n",
    "\n",
    "df_resampled_train = df_resampled_train.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_resampled_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_resampled_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=25)\n",
    "xgb_classifier = XGBClassifier(n_estimators=300, learning_rate=0.3,  random_state=25)\n",
    "logreg_classifier = LogisticRegression(C=0.1, penalty='none', random_state=25)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "\n",
    "names = ['Random Forest',\n",
    "         'XGBoost',\n",
    "         'Logistic Regression',\n",
    "         'Voting Classifier']\n",
    "\n",
    "model_accuracy_test_set = []\n",
    "model_accuracy_cross_val = []\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    # Evaluate on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test_set = model.predict(X_test)\n",
    "    test_set_score = accuracy_score(y_test, y_pred_test_set)\n",
    "    model_accuracy_test_set.append(test_set_score)\n",
    "\n",
    "    # Evaluate using cross-validation on the training set\n",
    "    cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=27)\n",
    "    cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cross_val_score_mean = cross_val_results.mean()\n",
    "    model_accuracy_cross_val.append(cross_val_score_mean)\n",
    "\n",
    "evaluation = pd.DataFrame({\n",
    "    'Models': names,\n",
    "    'Model Accuracy (Test Set)': model_accuracy_test_set,\n",
    "    'Cross-Validation Accuracy': model_accuracy_cross_val\n",
    "})\n",
    "\n",
    "evaluation.sort_values(by=\"Model Accuracy (Test Set)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "443b038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.4720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.51      0.62       107\n",
      "           1       0.07      0.22      0.11        18\n",
      "\n",
      "    accuracy                           0.47       125\n",
      "   macro avg       0.43      0.37      0.37       125\n",
      "weighted avg       0.69      0.47      0.55       125\n",
      "\n",
      "[[55 52]\n",
      " [14  4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Score evaluation of tuned GBC final model\n",
    "finalRF = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=25)\n",
    "\n",
    "finalRF.fit(X_train,y_train)\n",
    "RFpred = finalRF.predict(X_test)\n",
    "\n",
    "print('Accuracy on test set: {:.4f}'.format(finalRF.score(X_test, y_test)))\n",
    "print(classification_report(y_test, RFpred))\n",
    "print(confusion_matrix(y_test, RFpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae243eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "#dataframe\n",
    "CM_RF = confusion_matrix(y_test,RFpred)\n",
    "df_RF = pd.DataFrame(CM_RF)\n",
    "df_RF\n",
    "ax = sn.heatmap(df_RF,annot=True,fmt='.20g',cmap='Blues')\n",
    "ax.set_title('Confusion Matrix - Random Forest')\n",
    "ax.set_xlabel('Predict') #x\n",
    "ax.set_ylabel('True') #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90dccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_train_1.csv')\n",
    "df_train_1 = df_train_1.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_val = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ML_COVID_val.csv')\n",
    "df_val = df_val.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_val = df_val.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_val = df_val['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train_1 = scaler.fit_transform(X_train_1)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c95a698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    372\n",
      "1    372\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=11)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_1, y_train_1)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled_1 = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled_1['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled_1.to_csv('oversampled_normalized_ML_COVID_train_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411079f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9204341150456138\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9012386381628925\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.8976703928115364\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.5681544649793538\n",
      "\n",
      "Best Model: XGBoost, Mean Cross-Validation F1 Score: 0.9204341150456138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted Validation F1 Scores:\n",
      "Model: XGBoost, Validation F1 Score: 0.4782608695652174\n",
      "Model: Random Forest, Validation F1 Score: 0.3902439024390244\n",
      "Model: Voting Classifier, Validation F1 Score: 0.35000000000000003\n",
      "Model: Logistic Regression, Validation F1 Score: 0.30303030303030304\n",
      "\n",
      "Sorted Validation Accuracy Scores:\n",
      "Model: XGBoost, Validation Accuracy Score: 0.6756756756756757\n",
      "Model: Random Forest, Validation Accuracy Score: 0.6621621621621622\n",
      "Model: Voting Classifier, Validation Accuracy Score: 0.6486486486486487\n",
      "Model: Logistic Regression, Validation Accuracy Score: 0.3783783783783784\n",
      "\n",
      "Best Model (Validation F1): XGBoost, Validation F1 Score: 0.4782608695652174\n",
      "Best Model (Validation Accuracy): XGBoost, Validation Accuracy Score: 0.6756756756756757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14)\n",
    "xgb_classifier = XGBClassifier(random_state=14)\n",
    "logreg_classifier = LogisticRegression(random_state=14)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 6-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order\n",
    "sorted_f1_scores = sorted(f1_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores:\")\n",
    "for name, mean_f1_score in sorted_f1_scores:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Choose the best model based on the highest F1 score\n",
    "best_model_name, best_model_f1 = sorted_f1_scores[0]\n",
    "print(f\"\\nBest Model: {best_model_name}, Mean Cross-Validation F1 Score: {best_model_f1}\")\n",
    "\n",
    "# Lists to store validation F1 scores and accuracy scores\n",
    "validation_f1_scores = []\n",
    "validation_accuracy_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    validation_f1_scores.append((name, f1))\n",
    "    validation_accuracy_scores.append((name, accuracy))\n",
    "\n",
    "# Sort and print Validation F1 scores in descending order\n",
    "sorted_validation_f1_scores = sorted(validation_f1_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Validation F1 Scores:\")\n",
    "for name, f1_score in sorted_validation_f1_scores:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_score}')\n",
    "\n",
    "# Sort and print Validation accuracy scores in descending order\n",
    "sorted_validation_accuracy_scores = sorted(validation_accuracy_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Validation Accuracy Scores:\")\n",
    "for name, accuracy_score in sorted_validation_accuracy_scores:\n",
    "    print(f'Model: {name}, Validation Accuracy Score: {accuracy_score}')\n",
    "\n",
    "# Choose the best model based on the highest Validation F1 score\n",
    "best_model_name, best_model_f1 = sorted_validation_f1_scores[0]\n",
    "print(f\"\\nBest Model (Validation F1): {best_model_name}, Validation F1 Score: {best_model_f1}\")\n",
    "\n",
    "# Choose the best model based on the highest Validation accuracy score\n",
    "best_model_name_acc, best_model_acc = sorted_validation_accuracy_scores[0]\n",
    "print(f\"Best Model (Validation Accuracy): {best_model_name_acc}, Validation Accuracy Score: {best_model_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f46f4c",
   "metadata": {},
   "source": [
    "### Run and output using F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9134457188789494\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.862409032789694\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.788632092416496\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6046170984923185\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: XGBoost, Validation F1 Score: 0.4782608695652174\n",
      "Model: Voting Classifier, Validation F1 Score: 0.358974358974359\n",
      "Model: Random Forest, Validation F1 Score: 0.35714285714285715\n",
      "Model: Logistic Regression, Validation F1 Score: 0.25806451612903225\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=14)\n",
    "xgb_classifier = XGBClassifier(max_depth=5, random_state=14, learning_rate=0.1)\n",
    "logreg_classifier = LogisticRegression(C=0.1, random_state=14)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train the model on the entire oversampled training set and test on validation set\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782950ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificstion Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76        52\n",
      "           1       0.39      0.32      0.35        22\n",
      "\n",
      "    accuracy                           0.65        74\n",
      "   macro avg       0.56      0.55      0.55        74\n",
      "weighted avg       0.63      0.65      0.64        74\n",
      "\n",
      "[[41 11]\n",
      " [15  7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print('Classificstion Report:')\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(confusion_matrix(y_val,y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37aac4",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66d1b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9068910773454166\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9044034233772941\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.8962221805607632\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6678743961352657\n",
      "Model: Random Forest, Validation F1 Score with Custom Threshold: 0.5538461538461539\n",
      "Model: XGBoost, Validation F1 Score with Custom Threshold: 0.6071428571428571\n",
      "Model: Logistic Regression, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: XGBoost, Validation F1 Score: 0.6071428571428571\n",
      "Model: Random Forest, Validation F1 Score: 0.5538461538461539\n",
      "Model: Logistic Regression, Validation F1 Score: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 5}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}  # Adjust the range as needed\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "best_class_weights = grid_search.best_params_['class_weight']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_val = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold \n",
    "    custom_threshold = 0.1\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_val_custom = (y_prob_val > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_val = f1_score(y_val, y_pred_val_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Validation F1 Score with Custom Threshold: {f1_val}')\n",
    "    \n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d422c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9162603234217381\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9145851049343925\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.899080700845194\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6684881602914389\n",
      "Model: Random Forest, Validation F1 Score with Custom Threshold: 0.5625\n",
      "Model: XGBoost, Validation F1 Score with Custom Threshold: 0.6181818181818182\n",
      "Model: Logistic Regression, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: XGBoost, Validation F1 Score: 0.6181818181818182\n",
      "Model: Random Forest, Validation F1 Score: 0.5625\n",
      "Model: Logistic Regression, Validation F1 Score: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}  # Adjust the range as needed\n",
    "\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "# grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "# best_class_weights = grid_search.best_params_['class_weight']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_val = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold (you can experiment with different values)\n",
    "    custom_threshold = 0.1\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_val_custom = (y_prob_val > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_val = f1_score(y_val, y_pred_val_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Validation F1 Score with Custom Threshold: {f1_val}')\n",
    "    \n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ffe9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Best F1 Score: 0.8144647272338854\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.35000000000000003\n",
      "\n",
      "Model: XGBoost\n",
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 200}\n",
      "Best F1 Score: 0.7906776801920516\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.5306122448979591\n",
      "\n",
      "Model: Logistic Regression\n",
      "Best Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1 Score: 0.6672619047619047\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.4583333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "# Initialize individual classifiers with default parameters\n",
    "rf_classifier = RandomForestClassifier(random_state=14)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = [\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10, 20]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.3, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'none'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model, name, param_grid in zip(models, model_names, param_grids):\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "    grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "    # Get the best parameters and print results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_score = grid_search.best_score_\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Best Parameters: {best_params}')\n",
    "    print(f'Best F1 Score: {best_f1_score}\\n')\n",
    "\n",
    "    # Train the model with the best parameters on the entire oversampled training set\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "\n",
    "    # Predict on the validation set and evaluate\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    print(f'Validation F1 Score with Best Parameters: {f1_val}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff648ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9145851049343925\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.911664250514649\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.899080700845194\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6666666666666666\n",
      "Model: Random Forest, Validation F1 Score with Custom Threshold: 0.5625\n",
      "Model: XGBoost, Validation F1 Score with Custom Threshold: 0.6181818181818182\n",
      "Model: Logistic Regression, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score with Custom Threshold: 0.4583333333333333\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: XGBoost, Validation F1 Score: 0.6181818181818182\n",
      "Model: Random Forest, Validation F1 Score: 0.5625\n",
      "Model: Logistic Regression, Validation F1 Score: 0.4583333333333333\n",
      "Model: Voting Classifier, Validation F1 Score: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}  # Adjust the range as needed\n",
    "\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "# grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "# best_class_weights = grid_search.best_params_['class_weight']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(C=0.1, random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_val = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold (you can experiment with different values)\n",
    "    custom_threshold = 0.1\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_val_custom = (y_prob_val > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_val = f1_score(y_val, y_pred_val_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Validation F1 Score with Custom Threshold: {f1_val}')\n",
    "    \n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79d53a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Best F1 Score: 0.8144647272338854\n",
      "\n",
      "Model: XGBoost\n",
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 200}\n",
      "Best F1 Score: 0.7906776801920516\n",
      "\n",
      "Model: Logistic Regression\n",
      "Best Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1 Score: 0.6672619047619047\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.4186046511627907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_1.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for the training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "# Initialize individual classifiers with default parameters\n",
    "rf_classifier = RandomForestClassifier(random_state=14)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression']\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = [\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10, 20]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.3, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'none'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Store the best models and their parameters\n",
    "best_models = []\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model, name, param_grid in zip(models, model_names, param_grids):\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "    grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "    # Get the best parameters and print results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_score = grid_search.best_score_\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Best Parameters: {best_params}')\n",
    "    print(f'Best F1 Score: {best_f1_score}\\n')\n",
    "\n",
    "    # Train the model with the best parameters on the entire oversampled training set\n",
    "    best_model = model.__class__(**best_params)\n",
    "    best_model.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Append the best model to the list\n",
    "    best_models.append((name, best_model))\n",
    "\n",
    "# Create a Voting Classifier with the best models\n",
    "voting_classifier_best = VotingClassifier(estimators=best_models, voting='soft')  \n",
    "\n",
    "# Train the ensemble model on the entire oversampled training set\n",
    "voting_classifier_best.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Predict on the validation set and evaluate\n",
    "y_pred_val_best = voting_classifier_best.predict(X_val)\n",
    "f1_val_best = f1_score(y_val, y_pred_val_best)\n",
    "print(f'Validation F1 Score with Best Parameters: {f1_val_best}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d6b4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ori/ML_COVID_train.csv')\n",
    "\n",
    "# Load your pre-split testing dataset into a pandas DataFrame\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_test = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/ori/ML_COVID_test.csv')\n",
    "\n",
    "df_train = df_train.drop('pt', axis=1)\n",
    "df_test = df_test.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_test = df_test.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_test = df_test['Neuropsychiatric symptoms-new']\n",
    "\n",
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0eb6ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    424\n",
      "1    424\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=74)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled.to_csv('oversampled_normalized_ML_COVID_train_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f5ef61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.902347796374586\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.899424110724588\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.8865570702797251\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6677223364829552\n",
      "Model: Random Forest, Test F1 Score with Custom Threshold: 0.15384615384615385\n",
      "Model: XGBoost, Test F1 Score with Custom Threshold: 0.1473684210526316\n",
      "Model: Logistic Regression, Test F1 Score with Custom Threshold: 0.2517482517482518\n",
      "Model: Voting Classifier, Test F1 Score with Custom Threshold: 0.14285714285714285\n",
      "\n",
      "Sorted F1 Scores on Test Set:\n",
      "Model: Logistic Regression, Test F1 Score: 0.2517482517482518\n",
      "Model: Random Forest, Test F1 Score: 0.15384615384615385\n",
      "Model: XGBoost, Test F1 Score: 0.1473684210526316\n",
      "Model: Voting Classifier, Test F1 Score: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_2.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_resampled_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_resampled_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 2}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=75, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=75, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=75, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=76)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_test = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold (you can experiment with different values)\n",
    "    custom_threshold = 0.5\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_test_custom = (y_prob_test > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_test = f1_score(y_test, y_pred_test_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Test F1 Score with Custom Threshold: {f1_test}')\n",
    "    \n",
    "    f1_scores_test.append((name, f1_test))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_test = sorted(f1_scores_test, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Test Set:\")\n",
    "for name, f1_test in sorted_f1_scores_test:\n",
    "    print(f'Model: {name}, Test F1 Score: {f1_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a101ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on test set: 0.1474\n",
      "accuracy score on test set: 0.3520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.35      0.48       107\n",
      "           1       0.09      0.39      0.15        18\n",
      "\n",
      "    accuracy                           0.35       125\n",
      "   macro avg       0.43      0.37      0.31       125\n",
      "weighted avg       0.67      0.35      0.43       125\n",
      "\n",
      "[[37 70]\n",
      " [11  7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 2}  # Assign higher weight to the minority class\n",
    "\n",
    "XGBmodel = XGBClassifier(random_state=75, scale_pos_weight=class_weights[1])\n",
    "# LRmodel = LogisticRegression(random_state=75, class_weight=class_weights)\n",
    "# VCmodel = VotingClassifier(estimators=[\n",
    "#     ('random_forest', rf_classifier),\n",
    "#     ('xgboost', xgb_classifier),\n",
    "#     ('logistic_regression', logreg_classifier)\n",
    "# ], voting='soft')\n",
    "\n",
    "XGBmodel.fit(X_train,y_train)\n",
    "# y_prob_test = XGBmodel.predict_proba(X_test)[:, 1]\n",
    "# custom_threshold = 0.5\n",
    "y_pred = XGBmodel.predict(X_test)\n",
    "# y_pred_test_custom = (y_prob_test > custom_threshold).astype(int)\n",
    "\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "# f1_test = f1_score(y_test, y_pred_test_custom)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print('f1 score on test set: {:.4f}'.format(f1_test))\n",
    "print('accuracy score on test set: {:.4f}'.format(accuracy_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b10bbdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "#dataframe\n",
    "CM_VC = confusion_matrix(y_test, y_pred_test_custom)\n",
    "df_VC = pd.DataFrame(CM_VC)\n",
    "df_VC\n",
    "ax = sn.heatmap(df_VC,annot=True,fmt='.20g',cmap='Blues')\n",
    "ax.set_title('Confusion Matrix - Voting Classifier')\n",
    "ax.set_xlabel('Predict') #x\n",
    "ax.set_ylabel('True') #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41deb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/test/ML_COVID_train_cleaned_validate.csv')\n",
    "df_train_1 = df_train_1.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_val = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/test/ML_COVID_val.csv')\n",
    "df_val = df_val.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_val = df_val.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_val = df_val['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train_1 = scaler.fit_transform(X_train_1)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e02aec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    320\n",
      "1    320\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=11)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_1, y_train_1)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled_1 = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled_1['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled_1.to_csv('oversampled_normalized_ML_COVID_train_cleaned_testing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21c6f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9115001976978027\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9051603440338178\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.9041805586220182\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6209562708085357\n",
      "\n",
      "Best Model: XGBoost, Mean Cross-Validation F1 Score: 0.9115001976978027\n",
      "\n",
      "Sorted Validation F1 Scores:\n",
      "Model: Logistic Regression, Validation F1 Score: 0.23529411764705882\n",
      "Model: Voting Classifier, Validation F1 Score: 0.18867924528301885\n",
      "Model: Random Forest, Validation F1 Score: 0.18181818181818182\n",
      "Model: XGBoost, Validation F1 Score: 0.13333333333333333\n",
      "\n",
      "Sorted Validation Accuracy Scores:\n",
      "Model: Logistic Regression, Validation Accuracy Score: 0.792\n",
      "Model: Voting Classifier, Validation Accuracy Score: 0.656\n",
      "Model: Random Forest, Validation Accuracy Score: 0.64\n",
      "Model: XGBoost, Validation Accuracy Score: 0.376\n",
      "\n",
      "Best Model (Validation F1): Logistic Regression, Validation F1 Score: 0.23529411764705882\n",
      "Best Model (Validation Accuracy): Logistic Regression, Validation Accuracy Score: 0.792\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_cleaned_testing.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14)\n",
    "xgb_classifier = XGBClassifier(random_state=14)\n",
    "logreg_classifier = LogisticRegression(random_state=14)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 6-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order\n",
    "sorted_f1_scores = sorted(f1_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores:\")\n",
    "for name, mean_f1_score in sorted_f1_scores:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Choose the best model based on the highest F1 score\n",
    "best_model_name, best_model_f1 = sorted_f1_scores[0]\n",
    "print(f\"\\nBest Model: {best_model_name}, Mean Cross-Validation F1 Score: {best_model_f1}\")\n",
    "\n",
    "# Lists to store validation F1 scores and accuracy scores\n",
    "validation_f1_scores = []\n",
    "validation_accuracy_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    validation_f1_scores.append((name, f1))\n",
    "    validation_accuracy_scores.append((name, accuracy))\n",
    "\n",
    "# Sort and print Validation F1 scores in descending order\n",
    "sorted_validation_f1_scores = sorted(validation_f1_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Validation F1 Scores:\")\n",
    "for name, f1_score in sorted_validation_f1_scores:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_score}')\n",
    "\n",
    "# Sort and print Validation accuracy scores in descending order\n",
    "sorted_validation_accuracy_scores = sorted(validation_accuracy_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Validation Accuracy Scores:\")\n",
    "for name, accuracy_score in sorted_validation_accuracy_scores:\n",
    "    print(f'Model: {name}, Validation Accuracy Score: {accuracy_score}')\n",
    "\n",
    "# Choose the best model based on the highest Validation F1 score\n",
    "best_model_name, best_model_f1 = sorted_validation_f1_scores[0]\n",
    "print(f\"\\nBest Model (Validation F1): {best_model_name}, Validation F1 Score: {best_model_f1}\")\n",
    "\n",
    "# Choose the best model based on the highest Validation accuracy score\n",
    "best_model_name_acc, best_model_acc = sorted_validation_accuracy_scores[0]\n",
    "print(f\"Best Model (Validation Accuracy): {best_model_name_acc}, Validation Accuracy Score: {best_model_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0c32a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 50}\n",
      "Best F1 Score: 0.850545880329723\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.1724137931034483\n",
      "\n",
      "Model: XGBoost\n",
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best F1 Score: 0.8212535438891383\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.13186813186813187\n",
      "\n",
      "Model: Logistic Regression\n",
      "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1 Score: 0.6350262568232642\n",
      "\n",
      "Validation F1 Score with Best Parameters: 0.21276595744680854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_cleaned_testing.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "# class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "# Initialize individual classifiers with default parameters\n",
    "rf_classifier = RandomForestClassifier(random_state=14)\n",
    "xgb_classifier = XGBClassifier(random_state=14)  # XGBoost uses `scale_pos_weight` for class weights\n",
    "logreg_classifier = LogisticRegression(random_state=14)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = [\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10, 20]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.3, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'none'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model, name, param_grid in zip(models, model_names, param_grids):\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "    grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "    # Get the best parameters and print results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_score = grid_search.best_score_\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Best Parameters: {best_params}')\n",
    "    print(f'Best F1 Score: {best_f1_score}\\n')\n",
    "\n",
    "    # Train the model with the best parameters on the entire oversampled training set\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "\n",
    "    # Predict on the validation set and evaluate\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    print(f'Validation F1 Score with Best Parameters: {f1_val}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f913418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9154617684339706\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9049852266839826\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.9041805586220182\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6209562708085357\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: Logistic Regression, Validation F1 Score: 0.23529411764705882\n",
      "Model: Voting Classifier, Validation F1 Score: 0.1851851851851852\n",
      "Model: Random Forest, Validation F1 Score: 0.18181818181818182\n",
      "Model: XGBoost, Validation F1 Score: 0.13186813186813187\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_cleaned_testing.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14)\n",
    "xgb_classifier = XGBClassifier(max_depth=5, random_state=14)\n",
    "logreg_classifier = LogisticRegression(random_state=14)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='hard')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train the model on the entire oversampled training set and test on validation set\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4662ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9144077662524593\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9125715007217066\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.8990998759352632\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6630789812287149\n",
      "Model: Random Forest, Validation F1 Score with Custom Threshold: 0.16\n",
      "Model: XGBoost, Validation F1 Score with Custom Threshold: 0.11764705882352941\n",
      "Model: Logistic Regression, Validation F1 Score with Custom Threshold: 0.2517482517482518\n",
      "Model: Voting Classifier, Validation F1 Score with Custom Threshold: 0.1411764705882353\n",
      "\n",
      "Sorted F1 Scores on Validation Set:\n",
      "Model: Logistic Regression, Validation F1 Score: 0.2517482517482518\n",
      "Model: Random Forest, Validation F1 Score: 0.16\n",
      "Model: Voting Classifier, Validation F1 Score: 0.1411764705882353\n",
      "Model: XGBoost, Validation F1 Score: 0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train_1 = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_cleaned_testing.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train_1 = df_resampled_train_1.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train_1 = df_resampled_train_1['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 3}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}  # Adjust the range as needed\n",
    "\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "# grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "# best_class_weights = grid_search.best_params_['class_weight']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train_1, y_train_1, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_val = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_val = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold (you can experiment with different values)\n",
    "    custom_threshold = 0.5\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_val_custom = (y_prob_val > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_val = f1_score(y_val, y_pred_val_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Validation F1 Score with Custom Threshold: {f1_val}')\n",
    "    \n",
    "    f1_scores_val.append((name, f1_val))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_val = sorted(f1_scores_val, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Validation Set:\")\n",
    "for name, f1_val in sorted_f1_scores_val:\n",
    "    print(f'Model: {name}, Validation F1 Score: {f1_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30c09921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/test/ML_COVID_train_cleaned.csv')\n",
    "\n",
    "# Load your pre-split testing dataset into a pandas DataFrame\n",
    "# Replace 'test_dataset.csv' with the actual name of your testing dataset\n",
    "df_test = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/test/ML_COVID_test.csv')\n",
    "\n",
    "df_train = df_train.drop('pt', axis=1)\n",
    "df_test = df_test.drop('pt', axis=1)\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Separate features (X) and target variable (y) for testing dataset\n",
    "X_test = df_test.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_test = df_test['Neuropsychiatric symptoms-new']\n",
    "\n",
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60788ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "0    427\n",
      "1    427\n",
      "Name: Neuropsychiatric symptoms-new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_names = ['deepSleepTime (hours)', 'sleep_duration (hours)', 'REMTime (hours)','shallowSleepTime (hours)', 'Oxygen level (SpO2)']\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=74)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=column_names), pd.Series(y_resampled, name='Neuropsychiatric symptoms-new')], axis=1)\n",
    "\n",
    "# Display the count of each class after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(df_resampled['Neuropsychiatric symptoms-new'].value_counts())\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "df_resampled.to_csv('oversampled_normalized_ML_COVID_train_cleaned_2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72cda3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted F1 Scores after Cross-Validation:\n",
      "Model: XGBoost, Mean Cross-Validation F1 Score: 0.9149559820607872\n",
      "Model: Voting Classifier, Mean Cross-Validation F1 Score: 0.9023442872293671\n",
      "Model: Random Forest, Mean Cross-Validation F1 Score: 0.8962234735069329\n",
      "Model: Logistic Regression, Mean Cross-Validation F1 Score: 0.6677075715469748\n",
      "Model: Random Forest, Test F1 Score with Custom Threshold: 0.35616438356164387\n",
      "Model: XGBoost, Test F1 Score with Custom Threshold: 0.3287671232876712\n",
      "Model: Logistic Regression, Test F1 Score with Custom Threshold: 0.4583333333333333\n",
      "Model: Voting Classifier, Test F1 Score with Custom Threshold: 0.4318181818181818\n",
      "\n",
      "Sorted F1 Scores on Test Set:\n",
      "Model: Logistic Regression, Test F1 Score: 0.4583333333333333\n",
      "Model: Voting Classifier, Test F1 Score: 0.4318181818181818\n",
      "Model: Random Forest, Test F1 Score: 0.35616438356164387\n",
      "Model: XGBoost, Test F1 Score: 0.3287671232876712\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data from a CSV file\n",
    "df_resampled_train = pd.read_csv('C:/Users/User/Downloads/WQD7002 Data Science Research Project/datasets/same_months/ML_COVID/oversampled_normalized_ML_COVID_train_cleaned_2_test.csv')\n",
    "\n",
    "# Separate features (X) and target variable (y) for training dataset\n",
    "X_train = df_resampled_train.drop('Neuropsychiatric symptoms-new', axis=1)\n",
    "y_train = df_resampled_train['Neuropsychiatric symptoms-new']\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 5}  # Assign higher weight to the minority class\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}  # Adjust the range as needed\n",
    "\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "# grid_search.fit(X_train_1, y_train_1)\n",
    "\n",
    "# best_class_weights = grid_search.best_params_['class_weight']\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')  # 'hard' means majority voting, you can also use 'soft' for weighted voting\n",
    "\n",
    "# List of models and their names\n",
    "models = [rf_classifier, xgb_classifier, logreg_classifier, voting_classifier]\n",
    "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier']\n",
    "\n",
    "# Step 4: Combine oversampled data and perform 5-fold cross-validation for each model\n",
    "cv = StratifiedKFold(n_splits=6, shuffle=True, random_state=16)\n",
    "\n",
    "# Using F1 score as the scoring metric\n",
    "f1_scores_cv = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=f1_scorer)\n",
    "    mean_f1_score = cross_val_results.mean()\n",
    "    f1_scores_cv.append((name, mean_f1_score))\n",
    "\n",
    "# Sort and print F1 scores in descending order after cross-validation\n",
    "sorted_f1_scores_cv = sorted(f1_scores_cv, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores after Cross-Validation:\")\n",
    "for name, mean_f1_score in sorted_f1_scores_cv:\n",
    "    print(f'Model: {name}, Mean Cross-Validation F1 Score: {mean_f1_score}')\n",
    "\n",
    "# Step 5: Train each model on the entire oversampled training set, test on validation set, and tune class weights/threshold\n",
    "f1_scores_test = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model on the entire oversampled training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Set a custom threshold (you can experiment with different values)\n",
    "    custom_threshold = 0.3\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on the custom threshold\n",
    "    y_pred_test_custom = (y_prob_test > custom_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate the model with the custom threshold\n",
    "    f1_test = f1_score(y_test, y_pred_test_custom)\n",
    "    \n",
    "    print(f'Model: {name}, Test F1 Score with Custom Threshold: {f1_test}')\n",
    "    \n",
    "    f1_scores_test.append((name, f1_test))\n",
    "\n",
    "# Sort and print F1 scores on the validation set in descending order\n",
    "sorted_f1_scores_test = sorted(f1_scores_test, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted F1 Scores on Test Set:\")\n",
    "for name, f1_test in sorted_f1_scores_test:\n",
    "    print(f'Model: {name}, Test F1 Score: {f1_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ade1b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on test set: 0.4318\n",
      "accuracy score on test set: 0.3243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.10      0.17        52\n",
      "           1       0.29      0.86      0.43        22\n",
      "\n",
      "    accuracy                           0.32        74\n",
      "   macro avg       0.46      0.48      0.30        74\n",
      "weighted avg       0.52      0.32      0.25        74\n",
      "\n",
      "[[ 5 47]\n",
      " [ 3 19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 5}  # Assign higher weight to the minority class\n",
    "\n",
    "# XGBmodel = XGBClassifier(random_state=75, scale_pos_weight=class_weights[1])\n",
    "# LRmodel = LogisticRegression(random_state=75, class_weight=class_weights)\n",
    "# Initialize individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=14, class_weight=class_weights)\n",
    "xgb_classifier = XGBClassifier(random_state=14, scale_pos_weight=class_weights[1])  # XGBoost uses `scale_pos_weight` for class weights)\n",
    "logreg_classifier = LogisticRegression(random_state=14, class_weight=class_weights)\n",
    "VCmodel = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('xgboost', xgb_classifier),\n",
    "    ('logistic_regression', logreg_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "VCmodel.fit(X_train,y_train)\n",
    "y_prob_test = VCmodel.predict_proba(X_test)[:, 1]\n",
    "custom_threshold = 0.3\n",
    "# y_pred = VCmodel.predict(X_test)\n",
    "y_pred_test_custom = (y_prob_test > custom_threshold).astype(int)\n",
    "\n",
    "# f1_test = f1_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred_test_custom)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test_custom)\n",
    "print('f1 score on test set: {:.4f}'.format(f1_test))\n",
    "print('accuracy score on test set: {:.4f}'.format(accuracy_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred_test_custom))\n",
    "print(confusion_matrix(y_test, y_pred_test_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Define class weights (you can adjust these values based on your dataset characteristics)\n",
    "class_weights = {0: 1, 1: 2}  # Assign higher weight to the minority class\n",
    "\n",
    "XGBmodel = XGBClassifier(random_state=75, scale_pos_weight=class_weights[1])\n",
    "# LRmodel = LogisticRegression(random_state=75, class_weight=class_weights)\n",
    "# VCmodel = VotingClassifier(estimators=[\n",
    "#     ('random_forest', rf_classifier),\n",
    "#     ('xgboost', xgb_classifier),\n",
    "#     ('logistic_regression', logreg_classifier)\n",
    "# ], voting='soft')\n",
    "\n",
    "XGBmodel.fit(X_train,y_train)\n",
    "# y_prob_test = XGBmodel.predict_proba(X_test)[:, 1]\n",
    "# custom_threshold = 0.5\n",
    "y_pred = XGBmodel.predict(X_test)\n",
    "# y_pred_test_custom = (y_prob_test > custom_threshold).astype(int)\n",
    "\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "# f1_test = f1_score(y_test, y_pred_test_custom)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print('f1 score on test set: {:.4f}'.format(f1_test))\n",
    "print('accuracy score on test set: {:.4f}'.format(accuracy_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
